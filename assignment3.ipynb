{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3495d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: cats and dogs are not allowed\n",
      "Document 2: cats and dogs are antagonistic\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "doc1 = \"cats and dogs are not allowed\"\n",
    "doc2 = \"cats and dogs are antagonistic\"\n",
    "documents = [doc1, doc2]\n",
    "print(\"Document 1:\", doc1)\n",
    "print(\"Document 2:\", doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ BAG OF WORDS (BoW) MODEL ============\n",
    "\n",
    "# Tokenize documents - break text into individual words\n",
    "doc1_tokens = doc1.lower().split()\n",
    "doc2_tokens = doc2.lower().split()\n",
    "print(\"Tokenized Document 1:\", doc1_tokens)\n",
    "print(\"Tokenized Document 2:\", doc2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary - unique words across all documents\n",
    "vocabulary = sorted(set(doc1_tokens + doc2_tokens))\n",
    "print(\"\\nVocabulary (all unique words):\", vocabulary)\n",
    "print(\"Vocabulary size:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature vectors manually - count word occurrences in each document\n",
    "# Each position in vector corresponds to a word in vocabulary\n",
    "def create_bow_vector(tokens, vocab):\n",
    "    # Initialize vector with zeros for each word in vocabulary\n",
    "    vector = [0] * len(vocab)\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            idx = vocab.index(token)\n",
    "            vector[idx] += 1\n",
    "    return vector\n",
    "\n",
    "vector1 = create_bow_vector(doc1_tokens, vocabulary)\n",
    "vector2 = create_bow_vector(doc2_tokens, vocabulary)\n",
    "\n",
    "print(\"\\nFeature Vector for Document 1:\")\n",
    "print(vector1)\n",
    "print(\"\\nFeature Vector for Document 2:\")\n",
    "print(vector2)\n",
    "\n",
    "# Display word-to-count mapping\n",
    "print(\"\\nWord-to-count mapping for Doc1:\")\n",
    "for word, count in zip(vocabulary, vector1):\n",
    "    if count > 0:\n",
    "        print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ GENSIM BoW MODEL ============\n",
    "\n",
    "# Create Gensim Dictionary - maps each unique word to an ID\n",
    "tokenized_docs = [doc1_tokens, doc2_tokens]\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "print(\"\\nGensim Dictionary (word -> ID mapping):\")\n",
    "print(dictionary.token2id)\n",
    "print(f\"\\nDictionary size: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404faacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2bow - converts document to bag of words representation (word_id, frequency)\n",
    "bow_doc1 = dictionary.doc2bow(doc1_tokens)\n",
    "bow_doc2 = dictionary.doc2bow(doc2_tokens)\n",
    "\n",
    "print(\"\\ndoc2bow for Document 1 (word_id, count):\")\n",
    "print(bow_doc1)\n",
    "print(\"\\ndoc2bow for Document 2 (word_id, count):\")\n",
    "print(bow_doc2)\n",
    "\n",
    "# Decode to show which words the IDs represent\n",
    "print(\"\\nDecoded BoW for Document 1:\")\n",
    "for word_id, freq in bow_doc1:\n",
    "    print(f\"  {dictionary[word_id]}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6c815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ N-GRAMS MODEL ============\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Generate n-grams - consecutive sequences of n words\n",
    "def generate_ngrams(tokens, n):\n",
    "    # Returns list of n-grams as tuples\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "# Bigrams (2-grams)\n",
    "bigrams_doc1 = generate_ngrams(doc1_tokens, 2)\n",
    "bigrams_doc2 = generate_ngrams(doc2_tokens, 2)\n",
    "\n",
    "print(\"\\n========== BIGRAMS (2-grams) ==========\")\n",
    "print(\"Document 1 bigrams:\", bigrams_doc1)\n",
    "print(\"Document 2 bigrams:\", bigrams_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6656162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams (3-grams)\n",
    "trigrams_doc1 = generate_ngrams(doc1_tokens, 3)\n",
    "trigrams_doc2 = generate_ngrams(doc2_tokens, 3)\n",
    "\n",
    "print(\"\\n========== TRIGRAMS (3-grams) ==========\")\n",
    "print(\"Document 1 trigrams:\", trigrams_doc1)\n",
    "print(\"Document 2 trigrams:\", trigrams_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ NEXT WORD PREDICTION USING N-GRAMS ============\n",
    "\n",
    "# Build n-gram language model - stores frequency of each n-gram\n",
    "def build_ngram_model(documents, n):\n",
    "    # Count occurrences of each n-gram across all documents\n",
    "    model = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        tokens = doc.lower().split()\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        for ngram in ngrams:\n",
    "            model[ngram] += 1\n",
    "    return model\n",
    "\n",
    "# Build conditional probability model for next word prediction\n",
    "def build_next_word_model(documents, n):\n",
    "    # Maps (n-1) words to possible next words with their counts\n",
    "    next_word_model = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for doc in documents:\n",
    "        tokens = doc.lower().split()\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            # Context: first (n-1) words\n",
    "            context = ngram[:-1]\n",
    "            # Next word: last word\n",
    "            next_word = ngram[-1]\n",
    "            next_word_model[context][next_word] += 1\n",
    "    \n",
    "    return next_word_model\n",
    "\n",
    "# Build bigram model for next word prediction\n",
    "bigram_model = build_next_word_model(documents, 2)\n",
    "\n",
    "print(\"\\n========== NEXT WORD PREDICTION MODEL ==========\")\n",
    "print(\"\\nBigram Model (context -> next_word: count):\")\n",
    "for context, next_words in bigram_model.items():\n",
    "    print(f\"\\nAfter '{' '.join(context)}':\")\n",
    "    for word, count in next_words.items():\n",
    "        print(f\"  -> '{word}': {count} time(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict next word given context - returns most likely next word\n",
    "def predict_next_word(context, model):\n",
    "    # Convert context string to tuple for lookup\n",
    "    context_tuple = tuple(context.lower().split())\n",
    "    \n",
    "    if context_tuple not in model:\n",
    "        return None, \"Context not found in model\"\n",
    "    \n",
    "    # Get all possible next words with their counts\n",
    "    next_words = model[context_tuple]\n",
    "    \n",
    "    # Find word with highest count\n",
    "    predicted_word = max(next_words, key=next_words.get)\n",
    "    count = next_words[predicted_word]\n",
    "    \n",
    "    return predicted_word, count\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\n========== NEXT WORD PREDICTIONS ==========\")\n",
    "\n",
    "test_contexts = [\"cats and\", \"and dogs\", \"dogs are\", \"are not\"]\n",
    "\n",
    "for context in test_contexts:\n",
    "    predicted, info = predict_next_word(context, bigram_model)\n",
    "    if predicted:\n",
    "        print(f\"\\nContext: '{context}'\")\n",
    "        print(f\"Predicted next word: '{predicted}' (appeared {info} time(s))\")\n",
    "    else:\n",
    "        print(f\"\\nContext: '{context}' - {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8395f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build trigram model for more context-aware predictions\n",
    "trigram_model = build_next_word_model(documents, 3)\n",
    "\n",
    "print(\"\\n========== TRIGRAM PREDICTIONS (2-word context) ==========\")\n",
    "print(\"\\nTrigram Model:\")\n",
    "for context, next_words in trigram_model.items():\n",
    "    print(f\"\\nAfter '{' '.join(context)}':\")\n",
    "    for word, count in next_words.items():\n",
    "        print(f\"  -> '{word}': {count} time(s)\")\n",
    "\n",
    "# Test with 2-word contexts\n",
    "test_contexts_trigram = [\"cats and dogs\", \"and dogs are\", \"dogs are not\"]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for context in test_contexts_trigram:\n",
    "    predicted, info = predict_next_word(context, trigram_model)\n",
    "    if predicted:\n",
    "        print(f\"\\nContext: '{context}'\")\n",
    "        print(f\"Predicted next word: '{predicted}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ USING SPACY FOR BoW ============\n",
    "\n",
    "# Load spaCy model for text processing (tokenization, lemmatization)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process documents with spaCy\n",
    "doc1_spacy = nlp(doc1)\n",
    "doc2_spacy = nlp(doc2)\n",
    "\n",
    "# Extract tokens (excluding punctuation and stopwords for cleaner analysis)\n",
    "doc1_tokens_spacy = [token.text.lower() for token in doc1_spacy if not token.is_punct]\n",
    "doc2_tokens_spacy = [token.text.lower() for token in doc2_spacy if not token.is_punct]\n",
    "\n",
    "print(\"\\n========== SPACY PROCESSING ==========\")\n",
    "print(\"\\nSpacy tokens Doc1:\", doc1_tokens_spacy)\n",
    "print(\"Spacy tokens Doc2:\", doc2_tokens_spacy)\n",
    "\n",
    "# Create vocabulary from spaCy tokens\n",
    "vocab_spacy = sorted(set(doc1_tokens_spacy + doc2_tokens_spacy))\n",
    "print(\"\\nSpacy Vocabulary:\", vocab_spacy)\n",
    "\n",
    "# Create BoW vectors\n",
    "vector1_spacy = create_bow_vector(doc1_tokens_spacy, vocab_spacy)\n",
    "vector2_spacy = create_bow_vector(doc2_tokens_spacy, vocab_spacy)\n",
    "\n",
    "print(\"\\nSpacy BoW Vector Doc1:\", vector1_spacy)\n",
    "print(\"Spacy BoW Vector Doc2:\", vector2_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8800d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ WORD2VEC DEMONSTRATION ============\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare training data - list of tokenized sentences\n",
    "training_data = [doc1_tokens, doc2_tokens]\n",
    "\n",
    "# Train Word2Vec model - creates dense vector representations for words\n",
    "# vector_size: dimension of word vectors\n",
    "# window: maximum distance between current and predicted word\n",
    "# min_count: ignores words with frequency less than this\n",
    "# sg: 0 for CBOW, 1 for Skip-gram\n",
    "word2vec_model = Word2Vec(sentences=training_data, vector_size=10, window=2, min_count=1, sg=0)\n",
    "\n",
    "print(\"\\n========== WORD2VEC ==========\")\n",
    "print(\"\\nWord2Vec Vocabulary:\", list(word2vec_model.wv.index_to_key))\n",
    "\n",
    "# Get vector representation for a word\n",
    "print(\"\\nVector for 'cats':\", word2vec_model.wv['cats'])\n",
    "print(\"Vector for 'dogs':\", word2vec_model.wv['dogs'])\n",
    "\n",
    "# Find similar words (with limited data, results may not be meaningful)\n",
    "print(\"\\nVector dimensions:\", word2vec_model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dabc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ DOC2VEC DEMONSTRATION ============\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Prepare tagged documents - each document needs a unique tag/ID\n",
    "tagged_docs = [\n",
    "    TaggedDocument(words=doc1_tokens, tags=['DOC1']),\n",
    "    TaggedDocument(words=doc2_tokens, tags=['DOC2'])\n",
    "]\n",
    "\n",
    "# Train Doc2Vec model - creates vector representations for entire documents\n",
    "# vector_size: dimension of document vectors\n",
    "# min_count: ignores words with frequency less than this\n",
    "# epochs: number of training iterations\n",
    "doc2vec_model = Doc2Vec(tagged_docs, vector_size=10, min_count=1, epochs=40)\n",
    "\n",
    "print(\"\\n========== DOC2VEC ==========\")\n",
    "print(\"\\nDoc2Vec Vocabulary:\", list(doc2vec_model.wv.index_to_key))\n",
    "\n",
    "# Get vector representation for documents\n",
    "doc1_vector = doc2vec_model.dv['DOC1']\n",
    "doc2_vector = doc2vec_model.dv['DOC2']\n",
    "\n",
    "print(\"\\nDocument 1 vector:\", doc1_vector)\n",
    "print(\"Document 2 vector:\", doc2_vector)\n",
    "\n",
    "# Calculate similarity between documents using cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cosine_sim = dot(doc1_vector, doc2_vector) / (norm(doc1_vector) * norm(doc2_vector))\n",
    "print(f\"\\nCosine similarity between documents: {cosine_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef149b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ SUMMARY COMPARISON ============\n",
    "\n",
    "print(\"\\n========== SUMMARY ==========\")\n",
    "print(\"\\n1. BAG OF WORDS (BoW):\")\n",
    "print(\"   - Represents documents as word frequency vectors\")\n",
    "print(\"   - Ignores word order and context\")\n",
    "print(\"   - Simple and fast\")\n",
    "print(f\"   - Doc1 vector: {vector1}\")\n",
    "print(f\"   - Doc2 vector: {vector2}\")\n",
    "\n",
    "print(\"\\n2. N-GRAMS:\")\n",
    "print(\"   - Captures word sequences and local context\")\n",
    "print(\"   - Bigrams capture 2-word patterns\")\n",
    "print(\"   - Trigrams capture 3-word patterns\")\n",
    "print(f\"   - Doc1 bigrams: {bigrams_doc1}\")\n",
    "print(f\"   - Enables next word prediction\")\n",
    "\n",
    "print(\"\\n3. NEXT WORD PREDICTION:\")\n",
    "print(\"   - Uses n-gram frequencies to predict next word\")\n",
    "print(\"   - Example: 'cats and' -> 'dogs' (from training data)\")\n",
    "\n",
    "print(\"\\n4. WORD2VEC:\")\n",
    "print(\"   - Dense vector representation of words\")\n",
    "print(\"   - Captures semantic relationships\")\n",
    "print(f\"   - Vector size: {word2vec_model.wv.vector_size}\")\n",
    "\n",
    "print(\"\\n5. DOC2VEC:\")\n",
    "print(\"   - Vector representation of entire documents\")\n",
    "print(\"   - Useful for document similarity\")\n",
    "print(f\"   - Document similarity: {cosine_sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
