{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a5a381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3495d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: cats and dogs are not allowed\n",
      "Document 2: cats and dogs are antagonistic\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "doc1 = \"cats and dogs are not allowed\"\n",
    "doc2 = \"cats and dogs are antagonistic\"\n",
    "documents = [doc1, doc2]\n",
    "print(\"Document 1:\", doc1)\n",
    "print(\"Document 2:\", doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf5ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Document 1: ['cats', 'and', 'dogs', 'are', 'not', 'allowed']\n",
      "Tokenized Document 2: ['cats', 'and', 'dogs', 'are', 'antagonistic']\n"
     ]
    }
   ],
   "source": [
    "# ============ BAG OF WORDS (BoW) MODEL ============\n",
    "\n",
    "# Tokenize documents - break text into individual words\n",
    "doc1_tokens = doc1.lower().split()\n",
    "doc2_tokens = doc2.lower().split()\n",
    "print(\"Tokenized Document 1:\", doc1_tokens)\n",
    "print(\"Tokenized Document 2:\", doc2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4191069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary (all unique words): ['allowed', 'and', 'antagonistic', 'are', 'cats', 'dogs', 'not']\n",
      "Vocabulary size: 7\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary - unique words across all documents\n",
    "vocabulary = sorted(set(doc1_tokens + doc2_tokens))\n",
    "print(\"\\nVocabulary (all unique words):\", vocabulary)\n",
    "print(\"Vocabulary size:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7774c34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Vector for Document 1:\n",
      "[1, 1, 0, 1, 1, 1, 1]\n",
      "\n",
      "Feature Vector for Document 2:\n",
      "[0, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "Word-to-count mapping for Doc1:\n",
      "  allowed: 1\n",
      "  and: 1\n",
      "  are: 1\n",
      "  cats: 1\n",
      "  dogs: 1\n",
      "  not: 1\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors manually - count word occurrences in each document\n",
    "# Each position in vector corresponds to a word in vocabulary\n",
    "def create_bow_vector(tokens, vocab):\n",
    "    # Initialize vector with zeros for each word in vocabulary\n",
    "    vector = [0] * len(vocab)\n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            idx = vocab.index(token)\n",
    "            vector[idx] += 1\n",
    "    return vector\n",
    "\n",
    "vector1 = create_bow_vector(doc1_tokens, vocabulary)\n",
    "vector2 = create_bow_vector(doc2_tokens, vocabulary)\n",
    "\n",
    "print(\"\\nFeature Vector for Document 1:\")\n",
    "print(vector1)\n",
    "print(\"\\nFeature Vector for Document 2:\")\n",
    "print(vector2)\n",
    "\n",
    "# Display word-to-count mapping\n",
    "print(\"\\nWord-to-count mapping for Doc1:\")\n",
    "for word, count in zip(vocabulary, vector1):\n",
    "    if count > 0:\n",
    "        print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acc0475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gensim Dictionary (word -> ID mapping):\n",
      "{'allowed': 0, 'and': 1, 'are': 2, 'cats': 3, 'dogs': 4, 'not': 5, 'antagonistic': 6}\n",
      "\n",
      "Dictionary size: 7\n"
     ]
    }
   ],
   "source": [
    "# ============ GENSIM BoW MODEL ============\n",
    "\n",
    "# Create Gensim Dictionary - maps each unique word to an ID\n",
    "tokenized_docs = [doc1_tokens, doc2_tokens]\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "print(\"\\nGensim Dictionary (word -> ID mapping):\")\n",
    "print(dictionary.token2id)\n",
    "print(f\"\\nDictionary size: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "404faacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "doc2bow for Document 1 (word_id, count):\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "\n",
      "doc2bow for Document 2 (word_id, count):\n",
      "[(1, 1), (2, 1), (3, 1), (4, 1), (6, 1)]\n",
      "\n",
      "doc2idx for Document 1: [3, 1, 4, 2, 5, 0]\n",
      "doc2idx for Document 2: [3, 1, 4, 2, 6]\n",
      "\n",
      "Updated dictionary size after add_documents: 9\n",
      "Updated token2id: {'allowed': 0, 'and': 1, 'are': 2, 'cats': 3, 'dogs': 4, 'not': 5, 'antagonistic': 6, 'birds': 7, 'coexist': 8}\n",
      "\n",
      "Decoded BoW for Document 1:\n",
      "  allowed: 1\n",
      "  and: 1\n",
      "  are: 1\n",
      "  cats: 1\n",
      "  dogs: 1\n",
      "  not: 1\n"
     ]
    }
   ],
   "source": [
    "# doc2bow - converts document to bag of words representation (word_id, frequency)\n",
    "bow_doc1 = dictionary.doc2bow(doc1_tokens)\n",
    "bow_doc2 = dictionary.doc2bow(doc2_tokens)\n",
    "\n",
    "print(\"\\ndoc2bow for Document 1 (word_id, count):\")\n",
    "print(bow_doc1)\n",
    "print(\"\\ndoc2bow for Document 2 (word_id, count):\")\n",
    "print(bow_doc2)\n",
    "\n",
    "# Gensim Function 2: doc2idx - map tokens to dictionary IDs (-1 for OOV)\n",
    "doc1_ids = dictionary.doc2idx(doc1_tokens)\n",
    "doc2_ids = dictionary.doc2idx(doc2_tokens)\n",
    "print(\"\\ndoc2idx for Document 1:\", doc1_ids)\n",
    "print(\"doc2idx for Document 2:\", doc2_ids)\n",
    "\n",
    "# Gensim Function 3: add_documents - update dictionary with new text\n",
    "extra_docs = [[\"cats\", \"and\", \"birds\", \"coexist\"]]\n",
    "dictionary_updated = corpora.Dictionary(tokenized_docs)\n",
    "dictionary_updated.add_documents(extra_docs)\n",
    "print(\"\\nUpdated dictionary size after add_documents:\", len(dictionary_updated))\n",
    "print(\"Updated token2id:\", dictionary_updated.token2id)\n",
    "\n",
    "# Decode to show which words the IDs represent\n",
    "print(\"\\nDecoded BoW for Document 1:\")\n",
    "for word_id, freq in bow_doc1:\n",
    "    print(f\"  {dictionary[word_id]}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb6c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== BIGRAMS (2-grams) ==========\n",
      "Document 1 bigrams: [('cats', 'and'), ('and', 'dogs'), ('dogs', 'are'), ('are', 'not'), ('not', 'allowed')]\n",
      "Document 2 bigrams: [('cats', 'and'), ('and', 'dogs'), ('dogs', 'are'), ('are', 'antagonistic')]\n"
     ]
    }
   ],
   "source": [
    "# ============ N-GRAMS MODEL ============\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Generate n-grams - consecutive sequences of n words\n",
    "def generate_ngrams(tokens, n):\n",
    "    # Returns list of n-grams as tuples\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "# Bigrams (2-grams)\n",
    "bigrams_doc1 = generate_ngrams(doc1_tokens, 2)\n",
    "bigrams_doc2 = generate_ngrams(doc2_tokens, 2)\n",
    "\n",
    "print(\"\\n========== BIGRAMS (2-grams) ==========\")\n",
    "print(\"Document 1 bigrams:\", bigrams_doc1)\n",
    "print(\"Document 2 bigrams:\", bigrams_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6656162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TRIGRAMS (3-grams) ==========\n",
      "Document 1 trigrams: [('cats', 'and', 'dogs'), ('and', 'dogs', 'are'), ('dogs', 'are', 'not'), ('are', 'not', 'allowed')]\n",
      "Document 2 trigrams: [('cats', 'and', 'dogs'), ('and', 'dogs', 'are'), ('dogs', 'are', 'antagonistic')]\n"
     ]
    }
   ],
   "source": [
    "# Trigrams (3-grams)\n",
    "trigrams_doc1 = generate_ngrams(doc1_tokens, 3)\n",
    "trigrams_doc2 = generate_ngrams(doc2_tokens, 3)\n",
    "\n",
    "print(\"\\n========== TRIGRAMS (3-grams) ==========\")\n",
    "print(\"Document 1 trigrams:\", trigrams_doc1)\n",
    "print(\"Document 2 trigrams:\", trigrams_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d80fac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== NEXT WORD PREDICTION MODEL ==========\n",
      "\n",
      "Bigram Model (context -> next_word: count):\n",
      "\n",
      "After 'cats':\n",
      "  -> 'and': 2 time(s)\n",
      "\n",
      "After 'and':\n",
      "  -> 'dogs': 2 time(s)\n",
      "\n",
      "After 'dogs':\n",
      "  -> 'are': 2 time(s)\n",
      "\n",
      "After 'are':\n",
      "  -> 'not': 1 time(s)\n",
      "  -> 'antagonistic': 1 time(s)\n",
      "\n",
      "After 'not':\n",
      "  -> 'allowed': 1 time(s)\n",
      "\n",
      "========== BIGRAM CONDITIONAL PROBABILITIES ==========\n",
      "\n",
      "P(next_word | cats):\n",
      "  P(and | cats) = 1.000\n",
      "\n",
      "P(next_word | and):\n",
      "  P(dogs | and) = 1.000\n",
      "\n",
      "P(next_word | dogs):\n",
      "  P(are | dogs) = 1.000\n",
      "\n",
      "P(next_word | are):\n",
      "  P(not | are) = 0.500\n",
      "  P(antagonistic | are) = 0.500\n",
      "\n",
      "P(next_word | not):\n",
      "  P(allowed | not) = 1.000\n"
     ]
    }
   ],
   "source": [
    "# ============ NEXT WORD PREDICTION USING N-GRAMS ============\n",
    "\n",
    "# Build n-gram language model - stores frequency of each n-gram\n",
    "def build_ngram_model(documents, n):\n",
    "    model = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        tokens = doc.lower().split()\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "        for ngram in ngrams:\n",
    "            model[ngram] += 1\n",
    "    return model\n",
    "\n",
    "# Build conditional probability model for next word prediction\n",
    "def build_next_word_model(documents, n):\n",
    "    next_word_model = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for doc in documents:\n",
    "        tokens = doc.lower().split()\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "\n",
    "        for ngram in ngrams:\n",
    "            context = ngram[:-1]\n",
    "            next_word = ngram[-1]\n",
    "            next_word_model[context][next_word] += 1\n",
    "\n",
    "    return next_word_model\n",
    "\n",
    "# Convert counts to conditional probabilities P(next_word | context)\n",
    "def build_next_word_probabilities(next_word_model):\n",
    "    prob_model = {}\n",
    "    for context, next_words in next_word_model.items():\n",
    "        total = sum(next_words.values())\n",
    "        prob_model[context] = {word: count / total for word, count in next_words.items()}\n",
    "    return prob_model\n",
    "\n",
    "# Build bigram model and probabilities\n",
    "bigram_model = build_next_word_model(documents, 2)\n",
    "bigram_prob_model = build_next_word_probabilities(bigram_model)\n",
    "\n",
    "print(\"\\n========== NEXT WORD PREDICTION MODEL ==========\")\n",
    "print(\"\\nBigram Model (context -> next_word: count):\")\n",
    "for context, next_words in bigram_model.items():\n",
    "    print(f\"\\nAfter '{' '.join(context)}':\")\n",
    "    for word, count in next_words.items():\n",
    "        print(f\"  -> '{word}': {count} time(s)\")\n",
    "\n",
    "print(\"\\n========== BIGRAM CONDITIONAL PROBABILITIES ==========\")\n",
    "for context, probs in bigram_prob_model.items():\n",
    "    print(f\"\\nP(next_word | {' '.join(context)}):\")\n",
    "    for word, prob in probs.items():\n",
    "        print(f\"  P({word} | {' '.join(context)}) = {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5001659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== NEXT WORD PREDICTIONS ==========\n",
      "\n",
      "Context: 'cats'\n",
      "Predicted next word: 'and'\n",
      "Count: 2, Probability: 1.000\n",
      "\n",
      "Context: 'and'\n",
      "Predicted next word: 'dogs'\n",
      "Count: 2, Probability: 1.000\n",
      "\n",
      "Context: 'dogs'\n",
      "Predicted next word: 'are'\n",
      "Count: 2, Probability: 1.000\n",
      "\n",
      "Context: 'are'\n",
      "Predicted next word: 'not'\n",
      "Count: 1, Probability: 0.500\n"
     ]
    }
   ],
   "source": [
    "# Predict next word given context - returns most likely next word and probability\n",
    "def predict_next_word(context, count_model, prob_model):\n",
    "    context_tuple = tuple(context.lower().split())\n",
    "\n",
    "    if context_tuple not in count_model:\n",
    "        return None, None, \"Context not found in model\"\n",
    "\n",
    "    next_words = count_model[context_tuple]\n",
    "    predicted_word = max(next_words, key=next_words.get)\n",
    "    count = next_words[predicted_word]\n",
    "    probability = prob_model[context_tuple][predicted_word]\n",
    "\n",
    "    return predicted_word, probability, count\n",
    "\n",
    "print(\"\\n========== NEXT WORD PREDICTIONS ==========\")\n",
    "\n",
    "# For bigram model, context length must be 1 word\n",
    "test_contexts = [\"cats\", \"and\", \"dogs\", \"are\"]\n",
    "\n",
    "for context in test_contexts:\n",
    "    predicted, prob, count = predict_next_word(context, bigram_model, bigram_prob_model)\n",
    "    if predicted:\n",
    "        print(f\"\\nContext: '{context}'\")\n",
    "        print(f\"Predicted next word: '{predicted}'\")\n",
    "        print(f\"Count: {count}, Probability: {prob:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nContext: '{context}' - {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8395f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== TRIGRAM PREDICTIONS (2-word context) ==========\n",
      "\n",
      "Trigram Model:\n",
      "\n",
      "After 'cats and':\n",
      "  -> 'dogs': 2 time(s)\n",
      "\n",
      "After 'and dogs':\n",
      "  -> 'are': 2 time(s)\n",
      "\n",
      "After 'dogs are':\n",
      "  -> 'not': 1 time(s)\n",
      "  -> 'antagonistic': 1 time(s)\n",
      "\n",
      "After 'are not':\n",
      "  -> 'allowed': 1 time(s)\n",
      "\n",
      "Trigram Conditional Probabilities:\n",
      "P(dogs | cats and) = 1.000\n",
      "P(are | and dogs) = 1.000\n",
      "P(not | dogs are) = 0.500\n",
      "P(antagonistic | dogs are) = 0.500\n",
      "P(allowed | are not) = 1.000\n",
      "\n",
      "Predictions:\n",
      "\n",
      "Context: 'cats and'\n",
      "Predicted next word: 'dogs'\n",
      "Count: 2, Probability: 1.000\n",
      "\n",
      "Context: 'and dogs'\n",
      "Predicted next word: 'are'\n",
      "Count: 2, Probability: 1.000\n",
      "\n",
      "Context: 'dogs are'\n",
      "Predicted next word: 'not'\n",
      "Count: 1, Probability: 0.500\n"
     ]
    }
   ],
   "source": [
    "# Build trigram model for more context-aware predictions\n",
    "trigram_model = build_next_word_model(documents, 3)\n",
    "trigram_prob_model = build_next_word_probabilities(trigram_model)\n",
    "\n",
    "print(\"\\n========== TRIGRAM PREDICTIONS (2-word context) ==========\")\n",
    "print(\"\\nTrigram Model:\")\n",
    "for context, next_words in trigram_model.items():\n",
    "    print(f\"\\nAfter '{' '.join(context)}':\")\n",
    "    for word, count in next_words.items():\n",
    "        print(f\"  -> '{word}': {count} time(s)\")\n",
    "\n",
    "print(\"\\nTrigram Conditional Probabilities:\")\n",
    "for context, probs in trigram_prob_model.items():\n",
    "    for word, prob in probs.items():\n",
    "        print(f\"P({word} | {' '.join(context)}) = {prob:.3f}\")\n",
    "\n",
    "# Test with 3-word contexts for trigram model\n",
    "test_contexts_trigram = [\"cats and\", \"and dogs\", \"dogs are\"]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for context in test_contexts_trigram:\n",
    "    predicted, prob, count = predict_next_word(context, trigram_model, trigram_prob_model)\n",
    "    if predicted:\n",
    "        print(f\"\\nContext: '{context}'\")\n",
    "        print(f\"Predicted next word: '{predicted}'\")\n",
    "        print(f\"Count: {count}, Probability: {prob:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nContext: '{context}' - {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "950a4393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\n",
      "========== SPACY PROCESSING ==========\n",
      "\n",
      "Spacy tokens Doc1: ['cats', 'and', 'dogs', 'are', 'not', 'allowed']\n",
      "Spacy tokens Doc2: ['cats', 'and', 'dogs', 'are', 'antagonistic']\n",
      "\n",
      "Spacy lemmas Doc1: ['cat', 'and', 'dog', 'be', 'not', 'allow']\n",
      "Spacy lemmas Doc2: ['cat', 'and', 'dog', 'be', 'antagonistic']\n",
      "\n",
      "Spacy POS tags Doc1: [('cats', 'NOUN'), ('and', 'CCONJ'), ('dogs', 'NOUN'), ('are', 'AUX'), ('not', 'PART'), ('allowed', 'VERB')]\n",
      "Spacy POS tags Doc2: [('cats', 'NOUN'), ('and', 'CCONJ'), ('dogs', 'NOUN'), ('are', 'AUX'), ('antagonistic', 'ADJ')]\n",
      "\n",
      "Spacy Vocabulary: ['allowed', 'and', 'antagonistic', 'are', 'cats', 'dogs', 'not']\n",
      "\n",
      "Spacy BoW Vector Doc1: [1, 1, 0, 1, 1, 1, 1]\n",
      "Spacy BoW Vector Doc2: [0, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# ============ USING SPACY FOR BoW ============\n",
    "\n",
    "# Load spaCy model for text processing (with graceful fallback)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    try:\n",
    "        from spacy.cli import download\n",
    "        download(\"en_core_web_sm\")\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except Exception:\n",
    "        nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process documents with spaCy\n",
    "doc1_spacy = nlp(doc1)\n",
    "doc2_spacy = nlp(doc2)\n",
    "\n",
    "# spaCy Function 1: Tokenization\n",
    "doc1_tokens_spacy = [token.text.lower() for token in doc1_spacy if not token.is_punct]\n",
    "doc2_tokens_spacy = [token.text.lower() for token in doc2_spacy if not token.is_punct]\n",
    "\n",
    "print(\"\\n========== SPACY PROCESSING ==========\")\n",
    "print(\"\\nSpacy tokens Doc1:\", doc1_tokens_spacy)\n",
    "print(\"Spacy tokens Doc2:\", doc2_tokens_spacy)\n",
    "\n",
    "# spaCy Function 2: Lemmatization (token.lemma_)\n",
    "lemmas_doc1 = [token.lemma_ if token.lemma_ else token.text.lower() for token in doc1_spacy if not token.is_punct]\n",
    "lemmas_doc2 = [token.lemma_ if token.lemma_ else token.text.lower() for token in doc2_spacy if not token.is_punct]\n",
    "print(\"\\nSpacy lemmas Doc1:\", lemmas_doc1)\n",
    "print(\"Spacy lemmas Doc2:\", lemmas_doc2)\n",
    "\n",
    "# spaCy Function 3: POS tagging (token.pos_)\n",
    "pos_doc1 = [(token.text, token.pos_ if token.pos_ else 'N/A') for token in doc1_spacy if not token.is_punct]\n",
    "pos_doc2 = [(token.text, token.pos_ if token.pos_ else 'N/A') for token in doc2_spacy if not token.is_punct]\n",
    "print(\"\\nSpacy POS tags Doc1:\", pos_doc1)\n",
    "print(\"Spacy POS tags Doc2:\", pos_doc2)\n",
    "\n",
    "# Create vocabulary from spaCy tokens\n",
    "vocab_spacy = sorted(set(doc1_tokens_spacy + doc2_tokens_spacy))\n",
    "print(\"\\nSpacy Vocabulary:\", vocab_spacy)\n",
    "\n",
    "# Create BoW vectors\n",
    "vector1_spacy = create_bow_vector(doc1_tokens_spacy, vocab_spacy)\n",
    "vector2_spacy = create_bow_vector(doc2_tokens_spacy, vocab_spacy)\n",
    "\n",
    "print(\"\\nSpacy BoW Vector Doc1:\", vector1_spacy)\n",
    "print(\"Spacy BoW Vector Doc2:\", vector2_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8800d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== WORD2VEC ==========\n",
      "\n",
      "Word2Vec Vocabulary: ['are', 'dogs', 'and', 'cats', 'antagonistic', 'allowed', 'not']\n",
      "\n",
      "Vector for 'cats': [-0.07511582 -0.00930042  0.09538119 -0.07319167 -0.02333769 -0.01937741\n",
      "  0.08077437 -0.05930896  0.00045162 -0.04753734]\n",
      "Vector for 'dogs': [ 0.07380552 -0.01533481 -0.0453664   0.06554095 -0.0486019  -0.0181603\n",
      "  0.02876598  0.00991878 -0.08285265 -0.09448878]\n",
      "\n",
      "Similarity(cats, dogs): -0.2113\n",
      "Most similar to 'cats': [('are', 0.10494355112314224), ('allowed', 0.09267307072877884), ('and', -0.1055101752281189)]\n",
      "\n",
      "Vector dimensions: 10\n"
     ]
    }
   ],
   "source": [
    "# ============ WORD2VEC DEMONSTRATION ============\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare training data - list of tokenized sentences\n",
    "training_data = [doc1_tokens, doc2_tokens]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=training_data, vector_size=10, window=2, min_count=1, sg=0)\n",
    "\n",
    "print(\"\\n========== WORD2VEC ==========\")\n",
    "print(\"\\nWord2Vec Vocabulary:\", list(word2vec_model.wv.index_to_key))\n",
    "\n",
    "# Get vector representation for words\n",
    "print(\"\\nVector for 'cats':\", word2vec_model.wv['cats'])\n",
    "print(\"Vector for 'dogs':\", word2vec_model.wv['dogs'])\n",
    "\n",
    "# Additional Gensim functions on Word2Vec vectors\n",
    "print(\"\\nSimilarity(cats, dogs):\", round(word2vec_model.wv.similarity('cats', 'dogs'), 4))\n",
    "print(\"Most similar to 'cats':\", word2vec_model.wv.most_similar('cats', topn=3))\n",
    "\n",
    "print(\"\\nVector dimensions:\", word2vec_model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84dabc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== DOC2VEC ==========\n",
      "\n",
      "Doc2Vec Vocabulary: ['are', 'dogs', 'and', 'cats', 'antagonistic', 'allowed', 'not']\n",
      "\n",
      "Document 1 vector: [-0.05262421 -0.06005096 -0.09982104  0.08610007  0.0357652   0.00221071\n",
      " -0.09950028 -0.05158884 -0.09844406  0.02011195]\n",
      "Document 2 vector: [ 0.02822869  0.04650216 -0.04359442 -0.03130666 -0.03087194 -0.08773335\n",
      "  0.02148127  0.09267059 -0.09591305 -0.03467903]\n",
      "\n",
      "Cosine similarity between documents: -0.0520\n"
     ]
    }
   ],
   "source": [
    "# ============ DOC2VEC DEMONSTRATION ============\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Prepare tagged documents - each document needs a unique tag/ID\n",
    "tagged_docs = [\n",
    "    TaggedDocument(words=doc1_tokens, tags=['DOC1']),\n",
    "    TaggedDocument(words=doc2_tokens, tags=['DOC2'])\n",
    "]\n",
    "\n",
    "# Train Doc2Vec model - creates vector representations for entire documents\n",
    "# vector_size: dimension of document vectors\n",
    "# min_count: ignores words with frequency less than this\n",
    "# epochs: number of training iterations\n",
    "doc2vec_model = Doc2Vec(tagged_docs, vector_size=10, min_count=1, epochs=40)\n",
    "\n",
    "print(\"\\n========== DOC2VEC ==========\")\n",
    "print(\"\\nDoc2Vec Vocabulary:\", list(doc2vec_model.wv.index_to_key))\n",
    "\n",
    "# Get vector representation for documents\n",
    "doc1_vector = doc2vec_model.dv['DOC1']\n",
    "doc2_vector = doc2vec_model.dv['DOC2']\n",
    "\n",
    "print(\"\\nDocument 1 vector:\", doc1_vector)\n",
    "print(\"Document 2 vector:\", doc2_vector)\n",
    "\n",
    "# Calculate similarity between documents using cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cosine_sim = dot(doc1_vector, doc2_vector) / (norm(doc1_vector) * norm(doc2_vector))\n",
    "print(f\"\\nCosine similarity between documents: {cosine_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef149b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== SUMMARY ==========\n",
      "\n",
      "1. BAG OF WORDS (BoW):\n",
      "   - Represents documents as word frequency vectors\n",
      "   - Includes manual BoW and Gensim Dictionary-based BoW\n",
      "   - Doc1 vector: [1, 1, 0, 1, 1, 1, 1]\n",
      "   - Doc2 vector: [0, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "2. DICTIONARY + DOC2BOW (GENSIM):\n",
      "   - Dictionary created using corpora.Dictionary\n",
      "   - doc2bow converts text to (word_id, frequency)\n",
      "   - Dictionary mapping: {'allowed': 0, 'and': 1, 'are': 2, 'cats': 3, 'dogs': 4, 'not': 5, 'antagonistic': 6}\n",
      "   - Doc1 doc2bow: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "\n",
      "3. GENSIM FUNCTIONS DEMONSTRATED (>=3):\n",
      "   - dictionary.doc2bow()\n",
      "   - dictionary.doc2idx()\n",
      "   - dictionary.add_documents()\n",
      "   - word2vec_model.wv.similarity(), wv.most_similar()\n",
      "\n",
      "4. SPACY FUNCTIONS DEMONSTRATED (>=3):\n",
      "   - Tokenization (token.text)\n",
      "   - Lemmatization (token.lemma_)\n",
      "   - POS tagging (token.pos_)\n",
      "\n",
      "5. N-GRAMS WITH PROBABILITIES:\n",
      "   - Bigrams and trigrams generated\n",
      "   - Conditional probabilities computed: P(next_word | context)\n",
      "   - Used for next-word prediction\n",
      "\n",
      "6. WORD2VEC AND DOC2VEC:\n",
      "   - Word2Vec creates dense word embeddings\n",
      "   - Doc2Vec creates document embeddings\n",
      "   - Word2Vec vector size: 10\n",
      "   - Document similarity: -0.0520\n"
     ]
    }
   ],
   "source": [
    "# ============ SUMMARY COMPARISON ============\n",
    "\n",
    "print(\"\\n========== SUMMARY ==========\")\n",
    "print(\"\\n1. BAG OF WORDS (BoW):\")\n",
    "print(\"   - Represents documents as word frequency vectors\")\n",
    "print(\"   - Includes manual BoW and Gensim Dictionary-based BoW\")\n",
    "print(f\"   - Doc1 vector: {vector1}\")\n",
    "print(f\"   - Doc2 vector: {vector2}\")\n",
    "\n",
    "print(\"\\n2. DICTIONARY + DOC2BOW (GENSIM):\")\n",
    "print(\"   - Dictionary created using corpora.Dictionary\")\n",
    "print(\"   - doc2bow converts text to (word_id, frequency)\")\n",
    "print(f\"   - Dictionary mapping: {dictionary.token2id}\")\n",
    "print(f\"   - Doc1 doc2bow: {bow_doc1}\")\n",
    "\n",
    "print(\"\\n3. GENSIM FUNCTIONS DEMONSTRATED (>=3):\")\n",
    "print(\"   - dictionary.doc2bow()\")\n",
    "print(\"   - dictionary.doc2idx()\")\n",
    "print(\"   - dictionary.add_documents()\")\n",
    "print(\"   - word2vec_model.wv.similarity(), wv.most_similar()\")\n",
    "\n",
    "print(\"\\n4. SPACY FUNCTIONS DEMONSTRATED (>=3):\")\n",
    "print(\"   - Tokenization (token.text)\")\n",
    "print(\"   - Lemmatization (token.lemma_)\")\n",
    "print(\"   - POS tagging (token.pos_)\")\n",
    "\n",
    "print(\"\\n5. N-GRAMS WITH PROBABILITIES:\")\n",
    "print(\"   - Bigrams and trigrams generated\")\n",
    "print(\"   - Conditional probabilities computed: P(next_word | context)\")\n",
    "print(\"   - Used for next-word prediction\")\n",
    "\n",
    "print(\"\\n6. WORD2VEC AND DOC2VEC:\")\n",
    "print(\"   - Word2Vec creates dense word embeddings\")\n",
    "print(\"   - Doc2Vec creates document embeddings\")\n",
    "print(f\"   - Word2Vec vector size: {word2vec_model.wv.vector_size}\")\n",
    "print(f\"   - Document similarity: {cosine_sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
