{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddf54e1",
   "metadata": {},
   "source": [
    "# Study and exploration of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6224787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4cdbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75bf67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34de43",
   "metadata": {},
   "source": [
    "### NLTK modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d1f5",
   "metadata": {},
   "source": [
    "corpora : a package containing modoles if example text<br>\n",
    "wordnet : interface to the WordNet lexical resource<br>\n",
    "chunk : identify short non-nested phrases in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4e0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f924f9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'abc.zip',\n",
       " 'alpino',\n",
       " 'alpino.zip',\n",
       " 'bcp47.zip',\n",
       " 'biocreative_ppi',\n",
       " 'biocreative_ppi.zip',\n",
       " 'brown',\n",
       " 'brown.zip',\n",
       " 'brown_tei',\n",
       " 'brown_tei.zip',\n",
       " 'cess_cat',\n",
       " 'cess_cat.zip',\n",
       " 'cess_esp',\n",
       " 'cess_esp.zip',\n",
       " 'chat80',\n",
       " 'chat80.zip',\n",
       " 'city_database',\n",
       " 'city_database.zip',\n",
       " 'cmudict',\n",
       " 'cmudict.zip',\n",
       " 'comparative_sentences',\n",
       " 'comparative_sentences.zip',\n",
       " 'comtrans.zip',\n",
       " 'conll2000',\n",
       " 'conll2000.zip',\n",
       " 'conll2002',\n",
       " 'conll2002.zip',\n",
       " 'conll2007.zip',\n",
       " 'crubadan',\n",
       " 'crubadan.zip',\n",
       " 'dependency_treebank',\n",
       " 'dependency_treebank.zip',\n",
       " 'dolch',\n",
       " 'dolch.zip',\n",
       " 'english_wordnet',\n",
       " 'english_wordnet.zip',\n",
       " 'europarl_raw',\n",
       " 'europarl_raw.zip',\n",
       " 'extended_omw.zip',\n",
       " 'floresta',\n",
       " 'floresta.zip',\n",
       " 'framenet_v15',\n",
       " 'framenet_v15.zip',\n",
       " 'framenet_v17',\n",
       " 'framenet_v17.zip',\n",
       " 'gazetteers',\n",
       " 'gazetteers.zip',\n",
       " 'genesis',\n",
       " 'genesis.zip',\n",
       " 'gutenberg',\n",
       " 'gutenberg.zip',\n",
       " 'ieer',\n",
       " 'ieer.zip',\n",
       " 'inaugural',\n",
       " 'inaugural.zip',\n",
       " 'indian',\n",
       " 'indian.zip',\n",
       " 'jeita.zip',\n",
       " 'kimmo',\n",
       " 'kimmo.zip',\n",
       " 'knbc.zip',\n",
       " 'lin_thesaurus',\n",
       " 'lin_thesaurus.zip',\n",
       " 'machado.zip',\n",
       " 'mac_morpho',\n",
       " 'mac_morpho.zip',\n",
       " 'masc_tagged.zip',\n",
       " 'mock_corpus',\n",
       " 'mock_corpus.zip',\n",
       " 'movie_reviews',\n",
       " 'movie_reviews.zip',\n",
       " 'mte_teip5',\n",
       " 'mte_teip5.zip',\n",
       " 'names',\n",
       " 'names.zip',\n",
       " 'nombank.1.0.zip',\n",
       " 'nonbreaking_prefixes',\n",
       " 'nonbreaking_prefixes.zip',\n",
       " 'nps_chat',\n",
       " 'nps_chat.zip',\n",
       " 'omw-1.4.zip',\n",
       " 'omw.zip',\n",
       " 'opinion_lexicon',\n",
       " 'opinion_lexicon.zip',\n",
       " 'panlex_swadesh.zip',\n",
       " 'paradigms',\n",
       " 'paradigms.zip',\n",
       " 'pe08',\n",
       " 'pe08.zip',\n",
       " 'pil',\n",
       " 'pil.zip',\n",
       " 'pl196x',\n",
       " 'pl196x.zip',\n",
       " 'ppattach',\n",
       " 'ppattach.zip',\n",
       " 'problem_reports',\n",
       " 'problem_reports.zip',\n",
       " 'product_reviews_1',\n",
       " 'product_reviews_1.zip',\n",
       " 'product_reviews_2',\n",
       " 'product_reviews_2.zip',\n",
       " 'propbank.zip',\n",
       " 'pros_cons',\n",
       " 'pros_cons.zip',\n",
       " 'ptb',\n",
       " 'ptb.zip',\n",
       " 'qc',\n",
       " 'qc.zip',\n",
       " 'reuters.zip',\n",
       " 'rte',\n",
       " 'rte.zip',\n",
       " 'semcor.zip',\n",
       " 'senseval',\n",
       " 'senseval.zip',\n",
       " 'sentence_polarity',\n",
       " 'sentence_polarity.zip',\n",
       " 'sentiwordnet',\n",
       " 'sentiwordnet.zip',\n",
       " 'shakespeare',\n",
       " 'shakespeare.zip',\n",
       " 'sinica_treebank',\n",
       " 'sinica_treebank.zip',\n",
       " 'smultron',\n",
       " 'smultron.zip',\n",
       " 'state_union',\n",
       " 'state_union.zip',\n",
       " 'stopwords',\n",
       " 'stopwords.zip',\n",
       " 'subjectivity',\n",
       " 'subjectivity.zip',\n",
       " 'swadesh',\n",
       " 'swadesh.zip',\n",
       " 'switchboard',\n",
       " 'switchboard.zip',\n",
       " 'timit',\n",
       " 'timit.zip',\n",
       " 'toolbox',\n",
       " 'toolbox.zip',\n",
       " 'treebank',\n",
       " 'treebank.zip',\n",
       " 'twitter_samples',\n",
       " 'twitter_samples.zip',\n",
       " 'udhr',\n",
       " 'udhr.zip',\n",
       " 'udhr2',\n",
       " 'udhr2.zip',\n",
       " 'unicode_samples',\n",
       " 'unicode_samples.zip',\n",
       " 'universal_treebanks_v20.zip',\n",
       " 'verbnet',\n",
       " 'verbnet.zip',\n",
       " 'verbnet3',\n",
       " 'verbnet3.zip',\n",
       " 'webtext',\n",
       " 'webtext.zip',\n",
       " 'wordnet.zip',\n",
       " 'wordnet2021.zip',\n",
       " 'wordnet2022',\n",
       " 'wordnet2022.zip',\n",
       " 'wordnet31.zip',\n",
       " 'wordnet_ic',\n",
       " 'wordnet_ic.zip',\n",
       " 'words',\n",
       " 'words.zip',\n",
       " 'ycoe',\n",
       " 'ycoe.zip']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all corpora in NLTK\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "corpora_dir = nltk.data.find(\"corpora\")\n",
    "all_corpora = os.listdir(corpora_dir)\n",
    "all_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc365eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42becfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68fa2ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Wall Street Journal>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd8778f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80da979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "172ccb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31da7c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1532076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df4ad0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "37360\n",
      "['The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())\n",
    "hamlet = gutenberg.words('shakespeare-hamlet.txt')\n",
    "print(len(hamlet))\n",
    "print(hamlet[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ac859",
   "metadata": {},
   "source": [
    "String processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff9f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n",
      "['The quick brown fox jumps over the lazy dog.', 'NLTK is a powerful library for natural language processing.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog. NLTK is a powerful library for natural language processing.\"\n",
    "tokens = nltk.word_tokenize(sentence) # also can use - WordPunctTokenizer\n",
    "print(tokens)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "sens = nltk.sent_tokenize(sentence)\n",
    "print(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee008d",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8938265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# more such libraries - SnowballStemmer, LancasterStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f85577",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7511cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f8221",
   "metadata": {},
   "source": [
    "POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50fa5455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('striped', 'JJ'),\n",
       " ('bats', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('hanging', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('feet', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('best', 'JJS')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045346ed",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1) Stopword removal\n",
    "2) Tokenization (words,sentences,punctuation)\n",
    "3) Stemming\n",
    "4) Lemmatization\n",
    "5) POS tagging (all)\n",
    "6) Removing punctuation\n",
    "7) Lowercasing\n",
    "8) split\n",
    "9) Additional functions\n",
    "10) WordNet(anything other lexical module) - document submission\n",
    "\n",
    "Explore any 2 corpus and execute suitable NLP commands to demonstrate Text-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "122ccce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, reuters\n",
    "brown_sample = ' '.join(brown.words()[:300])\n",
    "reuters_sample = ' '.join(reuters.words()[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b34761",
   "metadata": {},
   "source": [
    "### Task 1: Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c56afbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: {\"mustn't\", 'ma', 'as', 'needn', 'wouldn', 'did', 'more', \"she'd\", 'and', 'he', 'has', \"he's\", 'does', 'this', 'its', 'theirs', 'didn', 'mustn', \"haven't\", 'can', 'that', 'were', 'aren', 'but', 'a', \"couldn't\", 'most', 'until', 'm', 'hers', \"weren't\", \"hasn't\", 'to', 'we', 'his', \"that'll\", 'yourself', 'if', 's', 'an', \"needn't\", 'themselves', \"shouldn't\", \"they'll\", \"we're\", 'too', 'wasn', 'out', 'these', \"you've\", \"don't\", \"it's\", 'have', 'just', 'ourselves', 'shan', \"you'd\", 'it', 'at', 'been', \"you're\", 'me', 'again', 'was', 'will', 'against', \"hadn't\", 't', 'under', 'haven', 'my', 'below', 'shouldn', 'some', 'hasn', 've', \"wasn't\", 'they', 'what', 'whom', 'who', 'down', \"he'd\", 'own', 'should', 'not', \"wouldn't\", 'be', 'such', 'which', 'than', \"we'd\", \"doesn't\", 'through', 'myself', 'you', 'him', 'off', \"i'd\", 'am', \"i'll\", 'above', 'how', 'isn', \"they're\", 'there', 'with', \"you'll\", 'by', 'over', 'both', 'no', 'weren', 'only', 'or', 'your', 'on', 'while', 'where', 'll', 're', 'himself', 'o', \"we'll\", \"aren't\", \"he'll\", 'don', 'the', \"it'd\", \"mightn't\", \"shan't\", 'is', 'doing', \"won't\", 'so', 'herself', 'other', \"i've\", 'mightn', 'all', 'once', \"they'd\", \"they've\", 'from', \"we've\", 'very', 'up', 'are', 'now', 'into', 'ain', 'she', \"isn't\", 'here', 'itself', 'them', 'yourselves', \"i'm\", 'hadn', 'her', \"didn't\", 'y', 'd', 'in', 'do', 'their', 'because', 'further', 'any', 'having', 'few', 'being', 'had', 'same', 'before', \"should've\", 'then', 'ours', 'between', 'about', 'each', 'our', \"she'll\", 'doesn', 'why', 'for', 'yours', 'those', 'nor', 'after', 'during', \"she's\", 'when', 'couldn', \"it'll\", 'i', 'of', 'won'}\n",
      "Brown - Stopword Removal:\n",
      "Original: 300 words\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "After removal: 188 words\n",
      "['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place', '.', 'jury']\n",
      "\n",
      "Reuters - Stopword Removal:\n",
      "Original: 300 words\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.']\n",
      "After removal: 196 words\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'U', '.', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'U', '.', '.', 'Japan', 'raised', 'fears', 'among', 'many']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f'Stopwords: {stop_words}')\n",
    "brown_words = brown_sample.split()\n",
    "reuters_words = reuters_sample.split()\n",
    "\n",
    "brown_filtered = [w for w in brown_words if w.lower() not in stop_words]\n",
    "reuters_filtered = [w for w in reuters_words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Brown - Stopword Removal:\")\n",
    "print(f\"Original: {len(brown_words)} words\")\n",
    "print(brown_words[:20])\n",
    "print(f\"After removal: {len(brown_filtered)} words\")\n",
    "print(brown_filtered[:20])\n",
    "\n",
    "print(\"\\nReuters - Stopword Removal:\")\n",
    "print(f\"Original: {len(reuters_words)} words\")\n",
    "print(reuters_words[:20])\n",
    "print(f\"After removal: {len(reuters_filtered)} words\")\n",
    "print(reuters_filtered[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a515a5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'What', 'is', '?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['up','down']\n",
    "text = 'Hello! What is up?'\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "filtered = [w for w in words if w.lower() not in stop_words]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7cb560",
   "metadata": {},
   "source": [
    "### Task 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "098cd993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Tokenization:\n",
      "Word tokens: 302\n",
      "Sentence tokens: 11\n",
      "First 15 word tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election']\n",
      "\n",
      "Reuters - Tokenization:\n",
      "Word tokens: 305\n",
      "Sentence tokens: 20\n",
      "First 15 word tokens: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "brown_word_tokens = word_tokenize(brown_sample)\n",
    "brown_sent_tokens = sent_tokenize(brown_sample)\n",
    "\n",
    "reuters_word_tokens = word_tokenize(reuters_sample)\n",
    "reuters_sent_tokens = sent_tokenize(reuters_sample)\n",
    "\n",
    "print(\"Brown - Tokenization:\")\n",
    "print(f\"Word tokens: {len(brown_word_tokens)}\")\n",
    "print(f\"Sentence tokens: {len(brown_sent_tokens)}\")\n",
    "print(\"First 15 word tokens:\", brown_word_tokens[:15])\n",
    "\n",
    "print(\"\\nReuters - Tokenization:\")\n",
    "print(f\"Word tokens: {len(reuters_word_tokens)}\")\n",
    "print(f\"Sentence tokens: {len(reuters_sent_tokens)}\")\n",
    "print(\"First 15 word tokens:\", reuters_word_tokens[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e759b",
   "metadata": {},
   "source": [
    "### Task 3: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef01af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Stemming:\n",
      "The -> the\n",
      "Fulton -> fulton\n",
      "County -> counti\n",
      "Grand -> grand\n",
      "Jury -> juri\n",
      "said -> said\n",
      "Friday -> friday\n",
      "an -> an\n",
      "investigation -> investig\n",
      "of -> of\n",
      "\n",
      "Reuters - Stemming:\n",
      "ASIAN -> asian\n",
      "EXPORTERS -> export\n",
      "FEAR -> fear\n",
      "DAMAGE -> damag\n",
      "FROM -> from\n",
      "U -> u\n",
      "S -> s\n",
      "JAPAN -> japan\n",
      "RIFT -> rift\n",
      "Mounting -> mount\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "brown_words_for_stem = [w for w in brown_word_tokens if w.isalpha()][:20]\n",
    "reuters_words_for_stem = [w for w in reuters_word_tokens if w.isalpha()][:20]\n",
    "\n",
    "brown_stemmed = [(w, stemmer.stem(w)) for w in brown_words_for_stem]\n",
    "reuters_stemmed = [(w, stemmer.stem(w)) for w in reuters_words_for_stem]\n",
    "\n",
    "print(\"Brown - Stemming:\")\n",
    "for original, stemmed in brown_stemmed[:10]:\n",
    "    print(f\"{original} -> {stemmed}\")\n",
    "\n",
    "print(\"\\nReuters - Stemming:\")\n",
    "for original, stemmed in reuters_stemmed[:10]:\n",
    "    print(f\"{original} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fc651",
   "metadata": {},
   "source": [
    "### Task 4: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e2d5454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Lemmatization:\n",
      "The -> The\n",
      "Fulton -> Fulton\n",
      "County -> County\n",
      "Grand -> Grand\n",
      "Jury -> Jury\n",
      "said -> said\n",
      "Friday -> Friday\n",
      "an -> an\n",
      "investigation -> investigation\n",
      "of -> of\n",
      "\n",
      "Reuters - Lemmatization:\n",
      "ASIAN -> ASIAN\n",
      "EXPORTERS -> EXPORTERS\n",
      "FEAR -> FEAR\n",
      "DAMAGE -> DAMAGE\n",
      "FROM -> FROM\n",
      "U -> U\n",
      "S -> S\n",
      "JAPAN -> JAPAN\n",
      "RIFT -> RIFT\n",
      "Mounting -> Mounting\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "brown_lemmatized = [(w, lemmatizer.lemmatize(w)) for w in brown_words_for_stem]\n",
    "reuters_lemmatized = [(w, lemmatizer.lemmatize(w)) for w in reuters_words_for_stem]\n",
    "\n",
    "print(\"Brown - Lemmatization:\")\n",
    "for original, lemmatized in brown_lemmatized[:10]:\n",
    "    print(f\"{original} -> {lemmatized}\")\n",
    "\n",
    "print(\"\\nReuters - Lemmatization:\")\n",
    "for original, lemmatized in reuters_lemmatized[:10]:\n",
    "    print(f\"{original} -> {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a12d2",
   "metadata": {},
   "source": [
    "### Task 5: POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5393d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - POS Tagging:\n",
      "[('The', 'DT'), ('Fulton', 'NNP'), ('County', 'NNP'), ('Grand', 'NNP'), ('Jury', 'NNP'), ('said', 'VBD'), ('Friday', 'NNP'), ('an', 'DT'), ('investigation', 'NN'), ('of', 'IN'), ('Atlanta', 'NNP'), (\"'s\", 'POS'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'DT'), ('evidence', 'NN'), ('``', '``'), ('that', 'IN'), ('any', 'DT'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.'), ('The', 'DT'), ('jury', 'NN'), ('further', 'RB'), ('said', 'VBD')]\n",
      "\n",
      "Reuters - POS Tagging:\n",
      "[('ASIAN', 'NNP'), ('EXPORTERS', 'NNP'), ('FEAR', 'NNP'), ('DAMAGE', 'NNP'), ('FROM', 'NNP'), ('U', 'NNP'), ('.', '.'), ('S', 'NNP'), ('.-', 'JJ'), ('JAPAN', 'NNP'), ('RIFT', 'NNP'), ('Mounting', 'NNP'), ('trade', 'NN'), ('friction', 'NN'), ('between', 'IN'), ('the', 'DT'), ('U', 'NNP'), ('.', '.'), ('S', 'NNP'), ('.', '.'), ('And', 'CC'), ('Japan', 'NNP'), ('has', 'VBZ'), ('raised', 'VBN'), ('fears', 'NNS'), ('among', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('Asia', 'NNP'), (\"'\", 'POS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "brown_pos = pos_tag(brown_word_tokens[:30])\n",
    "reuters_pos = pos_tag(reuters_word_tokens[:30])\n",
    "\n",
    "print(\"Brown - POS Tagging:\")\n",
    "print(brown_pos)\n",
    "\n",
    "print(\"\\nReuters - POS Tagging:\")\n",
    "print(reuters_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac48dcc",
   "metadata": {},
   "source": [
    "### Task 6: Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e680e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Removing Punctuation:\n",
      "Original: 302 tokens\n",
      "After removal: 283 tokens\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', '``']\n",
      "\n",
      "Reuters - Removing Punctuation:\n",
      "Original: 305 tokens\n",
      "After removal: 263 tokens\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', 'S', 'And', 'Japan', 'has']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "brown_no_punct = [w for w in brown_word_tokens if w not in string.punctuation]\n",
    "reuters_no_punct = [w for w in reuters_word_tokens if w not in string.punctuation]\n",
    "\n",
    "print(\"Brown - Removing Punctuation:\")\n",
    "print(f\"Original: {len(brown_word_tokens)} tokens\")\n",
    "print(f\"After removal: {len(brown_no_punct)} tokens\")\n",
    "print(brown_no_punct[:20])\n",
    "\n",
    "print(\"\\nReuters - Removing Punctuation:\")\n",
    "print(f\"Original: {len(reuters_word_tokens)} tokens\")\n",
    "print(f\"After removal: {len(reuters_no_punct)} tokens\")\n",
    "print(reuters_no_punct[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5ea95",
   "metadata": {},
   "source": [
    "### Task 7: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f5c779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Lowercasing:\n",
      "Original: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election']\n",
      "Lowercased: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlanta', \"'s\", 'recent', 'primary', 'election']\n",
      "\n",
      "Reuters - Lowercasing:\n",
      "Original: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between']\n",
      "Lowercased: ['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between']\n"
     ]
    }
   ],
   "source": [
    "brown_lower = [w.lower() for w in brown_word_tokens[:30]]\n",
    "reuters_lower = [w.lower() for w in reuters_word_tokens[:30]]\n",
    "\n",
    "print(\"Brown - Lowercasing:\")\n",
    "print(\"Original:\", brown_word_tokens[:15])\n",
    "print(\"Lowercased:\", brown_lower[:15])\n",
    "\n",
    "print(\"\\nReuters - Lowercasing:\")\n",
    "print(\"Original:\", reuters_word_tokens[:15])\n",
    "print(\"Lowercased:\", reuters_lower[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a108a1",
   "metadata": {},
   "source": [
    "### Task 8: Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8615f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Split:\n",
      "Total tokens: 300\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "\n",
      "Reuters - Split:\n",
      "Total tokens: 300\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.']\n"
     ]
    }
   ],
   "source": [
    "brown_split = brown_sample.split()\n",
    "reuters_split = reuters_sample.split()\n",
    "\n",
    "print(\"Brown - Split:\")\n",
    "print(f\"Total tokens: {len(brown_split)}\")\n",
    "print(brown_split[:20])\n",
    "\n",
    "print(\"\\nReuters - Split:\")\n",
    "print(f\"Total tokens: {len(reuters_split)}\")\n",
    "print(reuters_split[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8765bf0",
   "metadata": {},
   "source": [
    "### Task 9: 4 Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "047a6640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. FREQUENCY DISTRIBUTION - Word Count & Common Words\n",
      "============================================================\n",
      "\n",
      "Brown Corpus:\n",
      "Total unique words: 104\n",
      "Most common words: [('jury', 8), ('said', 7), ('city', 6), ('election', 5), ('fulton', 4), ('atlanta', 3), ('county', 2), ('grand', 2), ('primary', 2), ('irregularities', 2)]\n",
      "\n",
      "Reuters Corpus:\n",
      "Total unique words: 115\n",
      "Most common words: [('u', 6), ('said', 6), ('japan', 4), ('tariffs', 4), ('japanese', 3), ('electronics', 3), ('asian', 2), ('exporters', 2), ('damage', 2), ('businessmen', 2)]\n",
      "\n",
      "============================================================\n",
      "2. N-GRAMS - Word Pattern Analysis\n",
      "============================================================\n",
      "\n",
      "Brown Corpus:\n",
      "Sample Bigrams (2-grams): [('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday'), ('friday', 'investigation'), ('investigation', 'atlanta'), ('atlanta', 'recent'), ('recent', 'primary'), ('primary', 'election')]\n",
      "Sample Trigrams (3-grams): [('fulton', 'county', 'grand'), ('county', 'grand', 'jury'), ('grand', 'jury', 'said'), ('jury', 'said', 'friday'), ('said', 'friday', 'investigation')]\n",
      "\n",
      "Reuters Corpus:\n",
      "Sample Bigrams (2-grams): [('asian', 'exporters'), ('exporters', 'fear'), ('fear', 'damage'), ('damage', 'u'), ('u', 'japan'), ('japan', 'rift'), ('rift', 'mounting'), ('mounting', 'trade'), ('trade', 'friction'), ('friction', 'u')]\n",
      "Sample Trigrams (3-grams): [('asian', 'exporters', 'fear'), ('exporters', 'fear', 'damage'), ('fear', 'damage', 'u'), ('damage', 'u', 'japan'), ('u', 'japan', 'rift')]\n",
      "\n",
      "Most common Brown bigrams: [(('jury', 'said'), 6), (('fulton', 'county'), 2), (('grand', 'jury'), 2), (('county', 'grand'), 1), (('said', 'friday'), 1)]\n",
      "Most common Reuters bigrams: [(('u', 'japan'), 2), (('businessmen', 'officials'), 2), (('u', 'said'), 2), (('japanese', 'electronics'), 2), (('asian', 'exporters'), 1)]\n",
      "\n",
      "============================================================\n",
      "3. COLLOCATIONS - Meaningful Word Pairs\n",
      "============================================================\n",
      "\n",
      "Brown Corpus - Top Collocations (PMI):\n",
      "[('fulton', 'county'), ('grand', 'jury'), ('jury', 'said')]\n",
      "\n",
      "Reuters Corpus - Top Collocations (PMI):\n",
      "[('businessmen', 'officials'), ('japanese', 'electronics'), ('u', 'japan'), ('u', 'said')]\n",
      "\n",
      "============================================================\n",
      "4. CHUNKING - Phrase Extraction\n",
      "============================================================\n",
      "\n",
      "Brown Corpus - Extracted Chunks:\n",
      "NP: The Fulton County Grand Jury\n",
      "VP: said Friday\n",
      "NP: Friday\n",
      "NP: an investigation\n",
      "PP: of Atlanta\n",
      "NP: Atlanta\n",
      "NP: recent primary election\n",
      "NP: no evidence\n",
      "PP: that any irregularities\n",
      "NP: any irregularities\n",
      "VP: took place\n",
      "NP: place\n",
      "NP: The jury\n",
      "PP: in term-end presentments\n",
      "NP: term-end presentments\n",
      "PP: that the City Executive Committee\n",
      "NP: the City Executive Committee\n",
      "VP: had over-all charge\n",
      "NP: over-all charge\n",
      "PP: of the election\n",
      "NP: the election\n",
      "\n",
      "Reuters Corpus - Extracted Chunks:\n",
      "NP: ASIAN EXPORTERS FEAR DAMAGE FROM U\n",
      "NP: S\n",
      "NP: .- JAPAN RIFT Mounting trade friction\n",
      "PP: between the U\n",
      "NP: the U\n",
      "NP: S\n",
      "NP: Japan\n",
      "VP: raised fears\n",
      "NP: fears\n",
      "PP: of Asia\n",
      "NP: Asia\n",
      "NP: s\n",
      "VP: exporting nations\n",
      "NP: nations\n",
      "PP: that the row\n",
      "NP: the row\n",
      "VP: reaching economic damage\n",
      "NP: economic damage\n",
      "NP: businessmen\n",
      "NP: officials\n",
      "\n",
      "============================================================\n",
      "SUMMARY OF ADDITIONAL FUNCTIONS\n",
      "============================================================\n",
      "✓ Frequency Distribution: Analyzed word counts and identified most common words\n",
      "✓ N-grams: Extracted bigrams and trigrams for pattern analysis\n",
      "✓ Collocations: Found meaningful word pairs using PMI scoring\n",
      "✓ Chunking: Extracted noun phrases, verb phrases, and prepositional phrases\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist, ngrams, collocations\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Prepare data for analysis\n",
    "brown_tokens = nltk.word_tokenize(brown_sample)\n",
    "reuters_tokens = nltk.word_tokenize(reuters_sample)\n",
    "\n",
    "# Remove stopwords for better analysis\n",
    "stop_words = set(stopwords.words('english'))\n",
    "brown_filtered = [w.lower() for w in brown_tokens if w.isalpha() and w.lower() not in stop_words]\n",
    "reuters_filtered = [w.lower() for w in reuters_tokens if w.isalpha() and w.lower() not in stop_words]\n",
    "\n",
    "# 1. Frequency Distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"1. FREQUENCY DISTRIBUTION - Word Count & Common Words\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "brown_freq = FreqDist(brown_filtered)\n",
    "reuters_freq = FreqDist(reuters_filtered)\n",
    "\n",
    "print(\"\\nBrown Corpus:\")\n",
    "print(f\"Total unique words: {len(brown_freq)}\")\n",
    "print(f\"Most common words: {brown_freq.most_common(10)}\")\n",
    "\n",
    "print(\"\\nReuters Corpus:\")\n",
    "print(f\"Total unique words: {len(reuters_freq)}\")\n",
    "print(f\"Most common words: {reuters_freq.most_common(10)}\")\n",
    "\n",
    "# 2. N-grams - Word Pattern Analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. N-GRAMS - Word Pattern Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "brown_bigrams = list(ngrams(brown_filtered, 2))\n",
    "brown_trigrams = list(ngrams(brown_filtered, 3))\n",
    "reuters_bigrams = list(ngrams(reuters_filtered, 2))\n",
    "reuters_trigrams = list(ngrams(reuters_filtered, 3))\n",
    "\n",
    "print(\"\\nBrown Corpus:\")\n",
    "print(f\"Sample Bigrams (2-grams): {brown_bigrams[:10]}\")\n",
    "print(f\"Sample Trigrams (3-grams): {brown_trigrams[:5]}\")\n",
    "\n",
    "print(\"\\nReuters Corpus:\")\n",
    "print(f\"Sample Bigrams (2-grams): {reuters_bigrams[:10]}\")\n",
    "print(f\"Sample Trigrams (3-grams): {reuters_trigrams[:5]}\")\n",
    "\n",
    "# Most common bigrams\n",
    "brown_bigram_freq = FreqDist(brown_bigrams)\n",
    "reuters_bigram_freq = FreqDist(reuters_bigrams)\n",
    "print(f\"\\nMost common Brown bigrams: {brown_bigram_freq.most_common(5)}\")\n",
    "print(f\"Most common Reuters bigrams: {reuters_bigram_freq.most_common(5)}\")\n",
    "\n",
    "# 3. Collocations - Meaningful Word Pairs\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. COLLOCATIONS - Meaningful Word Pairs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use bigram collocation finder\n",
    "brown_bigram_measures = collocations.BigramAssocMeasures()\n",
    "brown_finder = collocations.BigramCollocationFinder.from_words(brown_filtered)\n",
    "brown_finder.apply_freq_filter(2)  # Only bigrams that appear at least 2 times\n",
    "\n",
    "reuters_bigram_measures = collocations.BigramAssocMeasures()\n",
    "reuters_finder = collocations.BigramCollocationFinder.from_words(reuters_filtered)\n",
    "reuters_finder.apply_freq_filter(2)\n",
    "\n",
    "print(\"\\nBrown Corpus - Top Collocations (PMI):\")\n",
    "print(brown_finder.nbest(brown_bigram_measures.pmi, 10))\n",
    "\n",
    "print(\"\\nReuters Corpus - Top Collocations (PMI):\")\n",
    "print(reuters_finder.nbest(reuters_bigram_measures.pmi, 10))\n",
    "\n",
    "# 4. Chunking - Phrase Extraction\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. CHUNKING - Phrase Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# POS tag the original tokens (with punctuation)\n",
    "brown_pos = nltk.pos_tag(brown_tokens[:50])\n",
    "reuters_pos = nltk.pos_tag(reuters_tokens[:50])\n",
    "\n",
    "# Define chunk grammar for noun phrases\n",
    "chunk_grammar = r\"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN.*>+}  # Noun phrases\n",
    "  VP: {<VB.*><NP|PP>}       # Verb phrases\n",
    "  PP: {<IN><NP>}            # Prepositional phrases\n",
    "\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "\n",
    "# Chunk the sentences\n",
    "brown_chunks = chunk_parser.parse(brown_pos)\n",
    "reuters_chunks = chunk_parser.parse(reuters_pos)\n",
    "\n",
    "print(\"\\nBrown Corpus - Extracted Chunks:\")\n",
    "for subtree in brown_chunks.subtrees(filter=lambda t: t.label() != 'S'):\n",
    "    print(f\"{subtree.label()}: {' '.join([word for word, tag in subtree.leaves()])}\")\n",
    "\n",
    "print(\"\\nReuters Corpus - Extracted Chunks:\")\n",
    "for subtree in reuters_chunks.subtrees(filter=lambda t: t.label() != 'S'):\n",
    "    print(f\"{subtree.label()}: {' '.join([word for word, tag in subtree.leaves()])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY OF ADDITIONAL FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Frequency Distribution: Analyzed word counts and identified most common words\")\n",
    "print(\"✓ N-grams: Extracted bigrams and trigrams for pattern analysis\")\n",
    "print(\"✓ Collocations: Found meaningful word pairs using PMI scoring\")\n",
    "print(\"✓ Chunking: Extracted noun phrases, verb phrases, and prepositional phrases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa2373",
   "metadata": {},
   "source": [
    "### Task 10: Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f94469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORDNET EXPLORATION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Word: 'BANK'\n",
      "============================================================\n",
      "\n",
      "Number of synsets: 18\n",
      "\n",
      "1. Synset: bank.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: sloping land (especially the slope beside a body of water)\n",
      "   Examples: ['they pulled the canoe up on the bank', 'he sat on the bank of the river and watched the currents']\n",
      "   Lemmas: ['bank']\n",
      "\n",
      "2. Synset: depository_financial_institution.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: a financial institution that accepts deposits and channels the money into lending activities\n",
      "   Examples: ['he cashed a check at the bank', 'that bank holds the mortgage on my home']\n",
      "   Lemmas: ['depository_financial_institution', 'bank', 'banking_concern', 'banking_company']\n",
      "\n",
      "3. Synset: bank.n.03\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: a long ridge or pile\n",
      "   Examples: ['a huge bank of earth']\n",
      "   Lemmas: ['bank']\n",
      "\n",
      "Synonyms of 'bank' (from first synset):\n",
      "   {'bank'}\n",
      "\n",
      "Antonyms of 'bank' (if available):\n",
      "   No direct antonyms found\n",
      "\n",
      "============================================================\n",
      "Word: 'GOOD'\n",
      "============================================================\n",
      "\n",
      "Number of synsets: 27\n",
      "\n",
      "1. Synset: good.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: benefit\n",
      "   Examples: ['for your own good', \"what's the good of worrying?\"]\n",
      "   Lemmas: ['good']\n",
      "\n",
      "2. Synset: good.n.02\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: moral excellence or admirableness\n",
      "   Examples: ['there is much good to be found in people']\n",
      "   Lemmas: ['good', 'goodness']\n",
      "\n",
      "3. Synset: good.n.03\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: that which is pleasing or valuable or useful\n",
      "   Examples: ['weigh the good against the bad', 'among the highest goods of all are happiness and self-realization']\n",
      "   Lemmas: ['good', 'goodness']\n",
      "\n",
      "Synonyms of 'good' (from first synset):\n",
      "   {'good'}\n",
      "\n",
      "Antonyms of 'good' (if available):\n",
      "   No direct antonyms found\n",
      "\n",
      "============================================================\n",
      "Word: 'RUN'\n",
      "============================================================\n",
      "\n",
      "Number of synsets: 57\n",
      "\n",
      "1. Synset: run.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: a score in baseball made by a runner touching all four bases safely\n",
      "   Examples: ['the Yankees scored 3 runs in the bottom of the 9th', 'their first tally came in the 3rd inning']\n",
      "   Lemmas: ['run', 'tally']\n",
      "\n",
      "2. Synset: test.n.05\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: the act of testing something\n",
      "   Examples: ['in the experimental trials the amount of carbon was measured separately', 'he called each flip of the coin a new trial']\n",
      "   Lemmas: ['test', 'trial', 'run']\n",
      "\n",
      "3. Synset: footrace.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: a race run on foot\n",
      "   Examples: ['she broke the record for the half-mile run']\n",
      "   Lemmas: ['footrace', 'foot_race', 'run']\n",
      "\n",
      "Synonyms of 'run' (from first synset):\n",
      "   {'tally', 'run'}\n",
      "\n",
      "Antonyms of 'run' (if available):\n",
      "   No direct antonyms found\n",
      "\n",
      "============================================================\n",
      "Word: 'COMPANY'\n",
      "============================================================\n",
      "\n",
      "Number of synsets: 10\n",
      "\n",
      "1. Synset: company.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: an institution created to conduct business\n",
      "   Examples: ['he only invests in large well-established companies', 'he started the company in his garage']\n",
      "   Lemmas: ['company']\n",
      "\n",
      "2. Synset: company.n.02\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: small military unit; usually two or three platoons\n",
      "   Examples: []\n",
      "   Lemmas: ['company']\n",
      "\n",
      "3. Synset: company.n.03\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: the state of being with someone\n",
      "   Examples: ['he missed their company', 'he enjoyed the society of his friends']\n",
      "   Lemmas: ['company', 'companionship', 'fellowship', 'society']\n",
      "\n",
      "Synonyms of 'company' (from first synset):\n",
      "   {'company'}\n",
      "\n",
      "Antonyms of 'company' (if available):\n",
      "   No direct antonyms found\n",
      "\n",
      "============================================================\n",
      "Word: 'MARKET'\n",
      "============================================================\n",
      "\n",
      "Number of synsets: 9\n",
      "\n",
      "1. Synset: market.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: the world of commercial activity where goods and services are bought and sold\n",
      "   Examples: ['without competition there would be no market', 'they were driven from the marketplace']\n",
      "   Lemmas: ['market', 'marketplace', 'market_place']\n",
      "\n",
      "2. Synset: market.n.02\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: the customers for a particular product or service\n",
      "   Examples: ['before they publish any book they try to determine the size of the market for it']\n",
      "   Lemmas: ['market']\n",
      "\n",
      "3. Synset: grocery_store.n.01\n",
      "   POS: n (n=noun, v=verb, a=adjective, r=adverb)\n",
      "   Definition: a marketplace where groceries are sold\n",
      "   Examples: ['the grocery store included a meat market']\n",
      "   Lemmas: ['grocery_store', 'grocery', 'food_market', 'market']\n",
      "\n",
      "Synonyms of 'market' (from first synset):\n",
      "   {'market', 'marketplace', 'market_place'}\n",
      "\n",
      "Antonyms of 'market' (if available):\n",
      "   No direct antonyms found\n",
      "\n",
      "============================================================\n",
      "SEMANTIC SIMILARITY\n",
      "============================================================\n",
      "\n",
      "Similarity between 'bank' and 'river': 0.333\n",
      "   bank: sloping land (especially the slope beside a body of water)\n",
      "   river: a large natural stream of water (larger than a creek)\n",
      "\n",
      "Similarity between 'bank' and 'money': 0.154\n",
      "   bank: sloping land (especially the slope beside a body of water)\n",
      "   money: the most common medium of exchange; functions as legal tender\n",
      "\n",
      "Similarity between 'good' and 'bad': 0.667\n",
      "   good: benefit\n",
      "   bad: that which is below standard or expectations as of ethics or decency\n",
      "\n",
      "Similarity between 'run' and 'walk': 0.571\n",
      "   run: a score in baseball made by a runner touching all four bases safely\n",
      "   walk: the act of traveling by foot\n",
      "\n",
      "============================================================\n",
      "HYPERNYMS AND HYPONYMS\n",
      "============================================================\n",
      "\n",
      "Word: 'company'\n",
      "Synset: company.n.01\n",
      "Definition: an institution created to conduct business\n",
      "\n",
      "Hypernyms (more general):\n",
      "   - institution.n.01: an organization founded and united for a specific purpose\n",
      "\n",
      "Hyponyms (more specific):\n",
      "   - broadcasting_company.n.01: a company that manages tv or radio stations\n",
      "   - furniture_company.n.01: a company that sells furniture\n",
      "   - dot-com.n.01: a company that operates its business primarily on the internet using a URL that ends in `.com'\n",
      "   - closed_shop.n.01: a company that hires only union members\n",
      "   - think_tank.n.01: a company that does research for hire and issues reports on the implications\n",
      "\n",
      "============================================================\n",
      "WORDNET SUMMARY\n",
      "============================================================\n",
      "✓ Explored word senses and definitions\n",
      "✓ Found synonyms and antonyms\n",
      "✓ Calculated semantic similarity between words\n",
      "✓ Identified hypernyms (general) and hyponyms (specific)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORDNET EXPLORATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select some words from our corpora for analysis\n",
    "test_words = ['bank', 'good', 'run', 'company', 'market']\n",
    "\n",
    "for word in test_words:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Word: '{word.upper()}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get synsets (synonym sets)\n",
    "    synsets = wn.synsets(word)\n",
    "    print(f\"\\nNumber of synsets: {len(synsets)}\")\n",
    "    \n",
    "    # Show first 3 synsets with details\n",
    "    for i, syn in enumerate(synsets[:3], 1):\n",
    "        print(f\"\\n{i}. Synset: {syn.name()}\")\n",
    "        print(f\"   POS: {syn.pos()} (n=noun, v=verb, a=adjective, r=adverb)\")\n",
    "        print(f\"   Definition: {syn.definition()}\")\n",
    "        print(f\"   Examples: {syn.examples()}\")\n",
    "        print(f\"   Lemmas: {[lemma.name() for lemma in syn.lemmas()]}\")\n",
    "    \n",
    "    # Demonstrate synonyms and antonyms\n",
    "    if synsets:\n",
    "        first_synset = synsets[0]\n",
    "        print(f\"\\nSynonyms of '{word}' (from first synset):\")\n",
    "        synonyms = set()\n",
    "        for lemma in first_synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "        print(f\"   {synonyms}\")\n",
    "        \n",
    "        print(f\"\\nAntonyms of '{word}' (if available):\")\n",
    "        antonyms = set()\n",
    "        for lemma in first_synset.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.antonyms()[0].name())\n",
    "        print(f\"   {antonyms if antonyms else 'No direct antonyms found'}\")\n",
    "\n",
    "# Demonstrate semantic similarity\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SEMANTIC SIMILARITY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Compare similar words\n",
    "word_pairs = [\n",
    "    ('bank', 'river'),\n",
    "    ('bank', 'money'),\n",
    "    ('good', 'bad'),\n",
    "    ('run', 'walk')\n",
    "]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    \n",
    "    if synsets1 and synsets2:\n",
    "        # Use Wu-Palmer similarity (0 to 1, higher is more similar)\n",
    "        similarity = synsets1[0].wup_similarity(synsets2[0])\n",
    "        print(f\"\\nSimilarity between '{word1}' and '{word2}': {similarity:.3f}\")\n",
    "        print(f\"   {word1}: {synsets1[0].definition()}\")\n",
    "        print(f\"   {word2}: {synsets2[0].definition()}\")\n",
    "\n",
    "# Demonstrate hypernyms and hyponyms\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"HYPERNYMS AND HYPONYMS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "word = 'company'\n",
    "synsets = wn.synsets(word)\n",
    "if synsets:\n",
    "    syn = synsets[0]\n",
    "    print(f\"\\nWord: '{word}'\")\n",
    "    print(f\"Synset: {syn.name()}\")\n",
    "    print(f\"Definition: {syn.definition()}\")\n",
    "    \n",
    "    # Hypernyms (more general terms)\n",
    "    print(f\"\\nHypernyms (more general):\")\n",
    "    for hyper in syn.hypernyms()[:3]:\n",
    "        print(f\"   - {hyper.name()}: {hyper.definition()}\")\n",
    "    \n",
    "    # Hyponyms (more specific terms)\n",
    "    print(f\"\\nHyponyms (more specific):\")\n",
    "    for hypo in syn.hyponyms()[:5]:\n",
    "        print(f\"   - {hypo.name()}: {hypo.definition()}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"WORDNET SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"✓ Explored word senses and definitions\")\n",
    "print(\"✓ Found synonyms and antonyms\")\n",
    "print(\"✓ Calculated semantic similarity between words\")\n",
    "print(\"✓ Identified hypernyms (general) and hyponyms (specific)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7d49e",
   "metadata": {},
   "source": [
    "### Task 11: Additional Lexical resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15ad7bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADDITIONAL LEXICAL RESOURCES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "1. NAMES CORPUS\n",
      "============================================================\n",
      "\n",
      "Total male names: 2943\n",
      "Total female names: 5001\n",
      "Sample male names: ['Simmonds', 'Isa', 'Collins', 'Salmon', 'Antin', 'Philip', 'Wojciech', 'Barthel', 'Filip', 'Bradly']\n",
      "Sample female names: ['Davine', 'Ariadne', 'Arleen', 'Neda', 'Cordelie', 'Dorolice', 'Viola', 'Eolande', 'Lillian', 'Amelie']\n",
      "\n",
      "Name Detection:\n",
      "   'John' is a MALE name\n",
      "   'Alice' is a FEMALE name\n",
      "   'market' is NOT a common first name\n",
      "   'company' is NOT a common first name\n",
      "   'Michael' is a MALE name\n",
      "   'Sarah' is a FEMALE name\n",
      "\n",
      "============================================================\n",
      "2. WORDS CORPUS - English Dictionary\n",
      "============================================================\n",
      "\n",
      "Total words in dictionary: 235892\n",
      "Sample words: ['berther', 'augurate', 'Logris', 'doon', 'suaveness', 'arrayment', 'pachylosis', 'rookeried', 'summage', 'Cestraciontes', 'pugilistically', 'pleurectomy', 'urometer', 'doddypoll', 'velours']\n",
      "\n",
      "Spell Checking:\n",
      "   ✓ 'hello' is spelled correctly\n",
      "   ✗ 'wrld' is NOT in dictionary (possible misspelling)\n",
      "   ✓ 'company' is spelled correctly\n",
      "   ✗ 'businss' is NOT in dictionary (possible misspelling)\n",
      "   ✓ 'market' is spelled correctly\n",
      "   ✗ 'analyss' is NOT in dictionary (possible misspelling)\n",
      "\n",
      "Words starting with 'financi' (first 10):\n",
      "   ['financial', 'financialist', 'financially', 'financier', 'financiery', 'financist']\n",
      "\n",
      "============================================================\n",
      "3. CMU PRONOUNCING DICTIONARY\n",
      "============================================================\n",
      "\n",
      "Total words with pronunciations: 123455\n",
      "\n",
      "Phonetic Pronunciations:\n",
      "\n",
      "   'company':\n",
      "      1. K AH1 M P AH0 N IY0\n",
      "\n",
      "   'market':\n",
      "      1. M AA1 R K AH0 T\n",
      "      2. M AA1 R K IH0 T\n",
      "\n",
      "   'finance':\n",
      "      1. F AH0 N AE1 N S\n",
      "      2. F IH0 N AE1 N S\n",
      "      3. F AY1 N AE2 N S\n",
      "\n",
      "   'analysis':\n",
      "      1. AH0 N AE1 L AH0 S AH0 S\n",
      "      2. AH0 N AE1 L IH0 S IH0 S\n",
      "\n",
      "   'python':\n",
      "      1. P AY1 TH AA0 N\n",
      "\n",
      "============================================================\n",
      "RHYME DETECTION\n",
      "============================================================\n",
      "\n",
      "Finding words that rhyme with 'good':\n",
      "Rhyme pattern: UH1 D\n",
      "Rhyming words (from first 1000): []\n",
      "\n",
      "============================================================\n",
      "SYLLABLE COUNTING\n",
      "============================================================\n",
      "\n",
      "Syllable counts:\n",
      "   'company': 3 syllable(s)\n",
      "   'business': 2 syllable(s)\n",
      "   'analysis': 4 syllable(s)\n",
      "   'market': 2 syllable(s)\n",
      "   'financial': 3 syllable(s)\n",
      "   'economy': 4 syllable(s)\n",
      "\n",
      "============================================================\n",
      "LEXICAL RESOURCES SUMMARY\n",
      "============================================================\n",
      "✓ Names Corpus: Identified common first names (male/female)\n",
      "✓ Words Corpus: Dictionary lookup and spell checking\n",
      "✓ CMU Dictionary: Phonetic transcriptions, rhyme detection, syllable counting\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names, words, cmudict\n",
    "import random\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ADDITIONAL LEXICAL RESOURCES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. NAMES CORPUS - Common first names\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. NAMES CORPUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "\n",
    "print(f\"\\nTotal male names: {len(male_names)}\")\n",
    "print(f\"Total female names: {len(female_names)}\")\n",
    "print(f\"Sample male names: {random.sample(male_names, 10)}\")\n",
    "print(f\"Sample female names: {random.sample(female_names, 10)}\")\n",
    "\n",
    "# Check if a word is a name\n",
    "test_words = ['John', 'Alice', 'market', 'company', 'Michael', 'Sarah']\n",
    "print(\"\\nName Detection:\")\n",
    "for word in test_words:\n",
    "    is_male = word in male_names\n",
    "    is_female = word in female_names\n",
    "    if is_male:\n",
    "        print(f\"   '{word}' is a MALE name\")\n",
    "    elif is_female:\n",
    "        print(f\"   '{word}' is a FEMALE name\")\n",
    "    else:\n",
    "        print(f\"   '{word}' is NOT a common first name\")\n",
    "\n",
    "# 2. WORDS CORPUS - English dictionary words\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. WORDS CORPUS - English Dictionary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "english_words = set(words.words())\n",
    "print(f\"\\nTotal words in dictionary: {len(english_words)}\")\n",
    "print(f\"Sample words: {random.sample(list(english_words), 15)}\")\n",
    "\n",
    "# Spell checking\n",
    "test_words = ['hello', 'wrld', 'company', 'businss', 'market', 'analyss']\n",
    "print(\"\\nSpell Checking:\")\n",
    "for word in test_words:\n",
    "    if word.lower() in english_words:\n",
    "        print(f\"   ✓ '{word}' is spelled correctly\")\n",
    "    else:\n",
    "        print(f\"   ✗ '{word}' is NOT in dictionary (possible misspelling)\")\n",
    "\n",
    "# Find words by pattern\n",
    "print(\"\\nWords starting with 'financi' (first 10):\")\n",
    "financial_words = [w for w in english_words if w.lower().startswith('financi')]\n",
    "print(f\"   {sorted(financial_words)[:10]}\")\n",
    "\n",
    "# 3. CMU PRONOUNCING DICTIONARY - Phonetic transcriptions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. CMU PRONOUNCING DICTIONARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pronouncing_dict = cmudict.dict()\n",
    "print(f\"\\nTotal words with pronunciations: {len(pronouncing_dict)}\")\n",
    "\n",
    "# Get pronunciations\n",
    "test_words = ['company', 'market', 'finance', 'analysis', 'python']\n",
    "print(\"\\nPhonetic Pronunciations:\")\n",
    "for word in test_words:\n",
    "    if word.lower() in pronouncing_dict:\n",
    "        pronunciations = pronouncing_dict[word.lower()]\n",
    "        print(f\"\\n   '{word}':\")\n",
    "        for i, pron in enumerate(pronunciations, 1):\n",
    "            print(f\"      {i}. {' '.join(pron)}\")\n",
    "    else:\n",
    "        print(f\"\\n   '{word}': No pronunciation found\")\n",
    "\n",
    "# Find rhyming words\n",
    "def get_rhyme_part(phonemes):\n",
    "    \"\"\"Extract the part of pronunciation used for rhyming\"\"\"\n",
    "    for i, phoneme in enumerate(phonemes):\n",
    "        if phoneme[-1].isdigit():  # Find first stressed vowel\n",
    "            return phonemes[i:]\n",
    "    return phonemes\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RHYME DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "word = 'good'\n",
    "if word.lower() in pronouncing_dict:\n",
    "    target_pron = pronouncing_dict[word.lower()][0]\n",
    "    target_rhyme = get_rhyme_part(target_pron)\n",
    "    \n",
    "    print(f\"\\nFinding words that rhyme with '{word}':\")\n",
    "    print(f\"Rhyme pattern: {' '.join(target_rhyme)}\")\n",
    "    \n",
    "    rhymes = []\n",
    "    for w, prons in list(pronouncing_dict.items())[:1000]:  # Check first 1000 words\n",
    "        for pron in prons:\n",
    "            if get_rhyme_part(pron) == target_rhyme and w != word.lower():\n",
    "                rhymes.append(w)\n",
    "                break\n",
    "    \n",
    "    print(f\"Rhyming words (from first 1000): {rhymes[:10]}\")\n",
    "\n",
    "# Syllable counting\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SYLLABLE COUNTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def count_syllables(word):\n",
    "    \"\"\"Count syllables in a word using CMU dict\"\"\"\n",
    "    if word.lower() in pronouncing_dict:\n",
    "        # Count vowel phonemes (those ending in digits)\n",
    "        return len([ph for ph in pronouncing_dict[word.lower()][0] if ph[-1].isdigit()])\n",
    "    return 0\n",
    "\n",
    "test_words = ['company', 'business', 'analysis', 'market', 'financial', 'economy']\n",
    "print(\"\\nSyllable counts:\")\n",
    "for word in test_words:\n",
    "    syllables = count_syllables(word)\n",
    "    print(f\"   '{word}': {syllables} syllable(s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LEXICAL RESOURCES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Names Corpus: Identified common first names (male/female)\")\n",
    "print(\"✓ Words Corpus: Dictionary lookup and spell checking\")\n",
    "print(\"✓ CMU Dictionary: Phonetic transcriptions, rhyme detection, syllable counting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
