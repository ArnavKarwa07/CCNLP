{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddf54e1",
   "metadata": {},
   "source": [
    "# Study and exploration of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6224787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c4cdbd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75bf67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34de43",
   "metadata": {},
   "source": [
    "### NLTK modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7d1f5",
   "metadata": {},
   "source": [
    "corpora : a package containing modoles if example text<br>\n",
    "wordnet : interface to the WordNet lexical resource<br>\n",
    "chunk : identify short non-nested phrases in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d4e0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f924f9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'abc.zip',\n",
       " 'alpino',\n",
       " 'alpino.zip',\n",
       " 'bcp47.zip',\n",
       " 'biocreative_ppi',\n",
       " 'biocreative_ppi.zip',\n",
       " 'brown',\n",
       " 'brown.zip',\n",
       " 'brown_tei',\n",
       " 'brown_tei.zip',\n",
       " 'cess_cat',\n",
       " 'cess_cat.zip',\n",
       " 'cess_esp',\n",
       " 'cess_esp.zip',\n",
       " 'chat80',\n",
       " 'chat80.zip',\n",
       " 'city_database',\n",
       " 'city_database.zip',\n",
       " 'cmudict',\n",
       " 'cmudict.zip',\n",
       " 'comparative_sentences',\n",
       " 'comparative_sentences.zip',\n",
       " 'comtrans.zip',\n",
       " 'conll2000',\n",
       " 'conll2000.zip',\n",
       " 'conll2002',\n",
       " 'conll2002.zip',\n",
       " 'conll2007.zip',\n",
       " 'crubadan',\n",
       " 'crubadan.zip',\n",
       " 'dependency_treebank',\n",
       " 'dependency_treebank.zip',\n",
       " 'dolch',\n",
       " 'dolch.zip',\n",
       " 'english_wordnet',\n",
       " 'english_wordnet.zip',\n",
       " 'europarl_raw',\n",
       " 'europarl_raw.zip',\n",
       " 'extended_omw.zip',\n",
       " 'floresta',\n",
       " 'floresta.zip',\n",
       " 'framenet_v15',\n",
       " 'framenet_v15.zip',\n",
       " 'framenet_v17',\n",
       " 'framenet_v17.zip',\n",
       " 'gazetteers',\n",
       " 'gazetteers.zip',\n",
       " 'genesis',\n",
       " 'genesis.zip',\n",
       " 'gutenberg',\n",
       " 'gutenberg.zip',\n",
       " 'ieer',\n",
       " 'ieer.zip',\n",
       " 'inaugural',\n",
       " 'inaugural.zip',\n",
       " 'indian',\n",
       " 'indian.zip',\n",
       " 'jeita.zip',\n",
       " 'kimmo',\n",
       " 'kimmo.zip',\n",
       " 'knbc.zip',\n",
       " 'lin_thesaurus',\n",
       " 'lin_thesaurus.zip',\n",
       " 'machado.zip',\n",
       " 'mac_morpho',\n",
       " 'mac_morpho.zip',\n",
       " 'masc_tagged.zip',\n",
       " 'mock_corpus',\n",
       " 'mock_corpus.zip',\n",
       " 'movie_reviews',\n",
       " 'movie_reviews.zip',\n",
       " 'mte_teip5',\n",
       " 'mte_teip5.zip',\n",
       " 'names',\n",
       " 'names.zip',\n",
       " 'nombank.1.0.zip',\n",
       " 'nonbreaking_prefixes',\n",
       " 'nonbreaking_prefixes.zip',\n",
       " 'nps_chat',\n",
       " 'nps_chat.zip',\n",
       " 'omw-1.4.zip',\n",
       " 'omw.zip',\n",
       " 'opinion_lexicon',\n",
       " 'opinion_lexicon.zip',\n",
       " 'panlex_swadesh.zip',\n",
       " 'paradigms',\n",
       " 'paradigms.zip',\n",
       " 'pe08',\n",
       " 'pe08.zip',\n",
       " 'pil',\n",
       " 'pil.zip',\n",
       " 'pl196x',\n",
       " 'pl196x.zip',\n",
       " 'ppattach',\n",
       " 'ppattach.zip',\n",
       " 'problem_reports',\n",
       " 'problem_reports.zip',\n",
       " 'product_reviews_1',\n",
       " 'product_reviews_1.zip',\n",
       " 'product_reviews_2',\n",
       " 'product_reviews_2.zip',\n",
       " 'propbank.zip',\n",
       " 'pros_cons',\n",
       " 'pros_cons.zip',\n",
       " 'ptb',\n",
       " 'ptb.zip',\n",
       " 'qc',\n",
       " 'qc.zip',\n",
       " 'reuters.zip',\n",
       " 'rte',\n",
       " 'rte.zip',\n",
       " 'semcor.zip',\n",
       " 'senseval',\n",
       " 'senseval.zip',\n",
       " 'sentence_polarity',\n",
       " 'sentence_polarity.zip',\n",
       " 'sentiwordnet',\n",
       " 'sentiwordnet.zip',\n",
       " 'shakespeare',\n",
       " 'shakespeare.zip',\n",
       " 'sinica_treebank',\n",
       " 'sinica_treebank.zip',\n",
       " 'smultron',\n",
       " 'smultron.zip',\n",
       " 'state_union',\n",
       " 'state_union.zip',\n",
       " 'stopwords',\n",
       " 'stopwords.zip',\n",
       " 'subjectivity',\n",
       " 'subjectivity.zip',\n",
       " 'swadesh',\n",
       " 'swadesh.zip',\n",
       " 'switchboard',\n",
       " 'switchboard.zip',\n",
       " 'timit',\n",
       " 'timit.zip',\n",
       " 'toolbox',\n",
       " 'toolbox.zip',\n",
       " 'treebank',\n",
       " 'treebank.zip',\n",
       " 'twitter_samples',\n",
       " 'twitter_samples.zip',\n",
       " 'udhr',\n",
       " 'udhr.zip',\n",
       " 'udhr2',\n",
       " 'udhr2.zip',\n",
       " 'unicode_samples',\n",
       " 'unicode_samples.zip',\n",
       " 'universal_treebanks_v20.zip',\n",
       " 'verbnet',\n",
       " 'verbnet.zip',\n",
       " 'verbnet3',\n",
       " 'verbnet3.zip',\n",
       " 'webtext',\n",
       " 'webtext.zip',\n",
       " 'wordnet.zip',\n",
       " 'wordnet2021.zip',\n",
       " 'wordnet2022',\n",
       " 'wordnet2022.zip',\n",
       " 'wordnet31.zip',\n",
       " 'wordnet_ic',\n",
       " 'wordnet_ic.zip',\n",
       " 'words',\n",
       " 'words.zip',\n",
       " 'ycoe',\n",
       " 'ycoe.zip']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all corpora in NLTK\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "corpora_dir = nltk.data.find(\"corpora\")\n",
    "all_corpora = os.listdir(corpora_dir)\n",
    "all_corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cfc365eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b42becfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1: Call me Ishmael .\n",
      "sent2: The family of Dashwood had long been settled in Sussex .\n",
      "sent3: In the beginning God created the heaven and the earth .\n",
      "sent4: Fellow - Citizens of the Senate and of the House of Representatives :\n",
      "sent5: I have a problem with people PMing me to lol JOIN\n",
      "sent6: SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there !\n",
      "sent7: Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "sent8: 25 SEXY MALE , seeks attrac older single lady , for discreet encounters .\n",
      "sent9: THE suburb of Saffron Park lay on the sunset side of London , as red and ragged as a cloud of sunset .\n"
     ]
    }
   ],
   "source": [
    "sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "68fa2ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Wall Street Journal>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd8778f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c80da979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "172ccb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31da7c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1532076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0df4ad0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "37360\n",
      "['The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())\n",
    "hamlet = gutenberg.words('shakespeare-hamlet.txt')\n",
    "print(len(hamlet))\n",
    "print(hamlet[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ac859",
   "metadata": {},
   "source": [
    "String processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eff9f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n",
      "['The quick brown fox jumps over the lazy dog.', 'NLTK is a powerful library for natural language processing.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog. NLTK is a powerful library for natural language processing.\"\n",
    "tokens = nltk.word_tokenize(sentence) # also can use - WordPunctTokenizer\n",
    "print(tokens)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)\n",
    "\n",
    "sens = nltk.sent_tokenize(sentence)\n",
    "print(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee008d",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8938265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# more such libraries - SnowballStemmer, LancasterStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f85577",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7511cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544f8221",
   "metadata": {},
   "source": [
    "POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50fa5455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('striped', 'JJ'),\n",
       " ('bats', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('hanging', 'VBG'),\n",
       " ('on', 'IN'),\n",
       " ('their', 'PRP$'),\n",
       " ('feet', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('best', 'JJS')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The striped bats are hanging on their feet for best\"\n",
    "nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045346ed",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1) Stopword removal\n",
    "2) Tokenization (words,sentences,punctuation)\n",
    "3) Stemming\n",
    "4) Lemmatization\n",
    "5) POS tagging (all)\n",
    "6) Removing punctuation\n",
    "7) Lowercasing\n",
    "8) split\n",
    "9) Additional functions\n",
    "10) WordNet(anything other lexical module) - document submission\n",
    "\n",
    "Explore any 2 corpus and execute suitable NLP commands to demonstrate Text-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "122ccce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, reuters\n",
    "brown_sample = ' '.join(brown.words()[:300])\n",
    "reuters_sample = ' '.join(reuters.words()[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b34761",
   "metadata": {},
   "source": [
    "### Task 1: Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c56afbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: {'about', \"they'd\", 'once', \"she'll\", 'then', 'all', \"i'm\", 'nor', \"it'll\", \"they've\", 'doing', 'weren', 'herself', 'again', 'haven', 'me', 't', 'when', 'y', 'a', 'their', 'it', 'does', 'ours', 'i', 'ain', 'hasn', 'only', 'ourselves', 'wasn', 'own', 'you', 'by', 'doesn', 'did', \"needn't\", 'below', 'over', 'at', 'yours', 'during', 'wouldn', 'such', \"i'd\", 'mustn', 'as', \"it'd\", 'most', 'just', 'because', 'they', 'any', \"he's\", \"we'll\", 'until', 'theirs', 'he', 'them', 'do', 'isn', 'itself', 'few', 'been', \"don't\", 'before', 'himself', 'what', 'that', 'into', 'yourselves', 'hadn', 'where', 'down', 'under', 'being', 'on', \"isn't\", 'hers', 'her', 're', \"shan't\", \"weren't\", 'these', 'more', 'my', 'm', 'its', 'couldn', 'which', 'were', 'o', 'd', \"you'll\", 'if', 'after', 'she', 'of', \"we're\", 'same', \"wasn't\", \"we've\", \"she'd\", 'aren', 'in', 'is', 'no', 'will', 'ma', 'whom', 'very', 'from', 'other', 'with', 'we', 'each', 'don', 'him', 've', 'his', \"aren't\", 'out', \"he'll\", \"wouldn't\", 'can', \"you're\", 'shouldn', \"shouldn't\", 'shan', \"won't\", 'those', 'there', 'but', 'll', \"couldn't\", 'mightn', \"haven't\", 'didn', \"i've\", 'some', \"doesn't\", 'had', \"you'd\", \"that'll\", 'both', \"mustn't\", 'have', 'not', 'above', 'against', 'off', \"should've\", 'are', \"hadn't\", \"hasn't\", \"mightn't\", \"she's\", 'so', \"they're\", 'through', 'too', \"we'd\", 'while', 'was', 's', \"didn't\", 'having', 'to', 'who', 'won', \"it's\", 'here', 'between', \"you've\", 'needn', 'should', \"he'd\", 'or', 'why', 'am', 'this', 'the', 'further', 'your', \"they'll\", 'for', 'myself', 'an', 'yourself', \"i'll\", 'than', 'and', 'up', 'has', 'how', 'be', 'themselves', 'our', 'now'}\n",
      "Brown - Stopword Removal:\n",
      "Original: 300 words\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "After removal: 188 words\n",
      "['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place', '.', 'jury']\n",
      "\n",
      "Reuters - Stopword Removal:\n",
      "Original: 300 words\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.']\n",
      "After removal: 196 words\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'U', '.', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'U', '.', '.', 'Japan', 'raised', 'fears', 'among', 'many']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f'Stopwords: {stop_words}')\n",
    "brown_words = brown_sample.split()\n",
    "reuters_words = reuters_sample.split()\n",
    "\n",
    "brown_filtered = [w for w in brown_words if w.lower() not in stop_words]\n",
    "reuters_filtered = [w for w in reuters_words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Brown - Stopword Removal:\")\n",
    "print(f\"Original: {len(brown_words)} words\")\n",
    "print(brown_words[:20])\n",
    "print(f\"After removal: {len(brown_filtered)} words\")\n",
    "print(brown_filtered[:20])\n",
    "\n",
    "print(\"\\nReuters - Stopword Removal:\")\n",
    "print(f\"Original: {len(reuters_words)} words\")\n",
    "print(reuters_words[:20])\n",
    "print(f\"After removal: {len(reuters_filtered)} words\")\n",
    "print(reuters_filtered[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a515a5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'What', 'is', '?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['up','down']\n",
    "text = 'Hello! What is up?'\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "filtered = [w for w in words if w.lower() not in stop_words]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7cb560",
   "metadata": {},
   "source": [
    "### Task 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "098cd993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Tokenization:\n",
      "Word tokens: 302\n",
      "Sentence tokens: 11\n",
      "First 15 word tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election']\n",
      "\n",
      "Reuters - Tokenization:\n",
      "Word tokens: 305\n",
      "Sentence tokens: 20\n",
      "First 15 word tokens: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "brown_word_tokens = word_tokenize(brown_sample)\n",
    "brown_sent_tokens = sent_tokenize(brown_sample)\n",
    "\n",
    "reuters_word_tokens = word_tokenize(reuters_sample)\n",
    "reuters_sent_tokens = sent_tokenize(reuters_sample)\n",
    "\n",
    "print(\"Brown - Tokenization:\")\n",
    "print(f\"Word tokens: {len(brown_word_tokens)}\")\n",
    "print(f\"Sentence tokens: {len(brown_sent_tokens)}\")\n",
    "print(\"First 15 word tokens:\", brown_word_tokens[:15])\n",
    "\n",
    "print(\"\\nReuters - Tokenization:\")\n",
    "print(f\"Word tokens: {len(reuters_word_tokens)}\")\n",
    "print(f\"Sentence tokens: {len(reuters_sent_tokens)}\")\n",
    "print(\"First 15 word tokens:\", reuters_word_tokens[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e759b",
   "metadata": {},
   "source": [
    "### Task 3: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ef01af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Stemming:\n",
      "The -> the\n",
      "Fulton -> fulton\n",
      "County -> counti\n",
      "Grand -> grand\n",
      "Jury -> juri\n",
      "said -> said\n",
      "Friday -> friday\n",
      "an -> an\n",
      "investigation -> investig\n",
      "of -> of\n",
      "\n",
      "Reuters - Stemming:\n",
      "ASIAN -> asian\n",
      "EXPORTERS -> export\n",
      "FEAR -> fear\n",
      "DAMAGE -> damag\n",
      "FROM -> from\n",
      "U -> u\n",
      "S -> s\n",
      "JAPAN -> japan\n",
      "RIFT -> rift\n",
      "Mounting -> mount\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "brown_words_for_stem = [w for w in brown_word_tokens if w.isalpha()][:20]\n",
    "reuters_words_for_stem = [w for w in reuters_word_tokens if w.isalpha()][:20]\n",
    "\n",
    "brown_stemmed = [(w, stemmer.stem(w)) for w in brown_words_for_stem]\n",
    "reuters_stemmed = [(w, stemmer.stem(w)) for w in reuters_words_for_stem]\n",
    "\n",
    "print(\"Brown - Stemming:\")\n",
    "for original, stemmed in brown_stemmed[:10]:\n",
    "    print(f\"{original} -> {stemmed}\")\n",
    "\n",
    "print(\"\\nReuters - Stemming:\")\n",
    "for original, stemmed in reuters_stemmed[:10]:\n",
    "    print(f\"{original} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42fc651",
   "metadata": {},
   "source": [
    "### Task 4: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e2d5454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Lemmatization:\n",
      "The -> The\n",
      "Fulton -> Fulton\n",
      "County -> County\n",
      "Grand -> Grand\n",
      "Jury -> Jury\n",
      "said -> said\n",
      "Friday -> Friday\n",
      "an -> an\n",
      "investigation -> investigation\n",
      "of -> of\n",
      "\n",
      "Reuters - Lemmatization:\n",
      "ASIAN -> ASIAN\n",
      "EXPORTERS -> EXPORTERS\n",
      "FEAR -> FEAR\n",
      "DAMAGE -> DAMAGE\n",
      "FROM -> FROM\n",
      "U -> U\n",
      "S -> S\n",
      "JAPAN -> JAPAN\n",
      "RIFT -> RIFT\n",
      "Mounting -> Mounting\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "brown_lemmatized = [(w, lemmatizer.lemmatize(w)) for w in brown_words_for_stem]\n",
    "reuters_lemmatized = [(w, lemmatizer.lemmatize(w)) for w in reuters_words_for_stem]\n",
    "\n",
    "print(\"Brown - Lemmatization:\")\n",
    "for original, lemmatized in brown_lemmatized[:10]:\n",
    "    print(f\"{original} -> {lemmatized}\")\n",
    "\n",
    "print(\"\\nReuters - Lemmatization:\")\n",
    "for original, lemmatized in reuters_lemmatized[:10]:\n",
    "    print(f\"{original} -> {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a12d2",
   "metadata": {},
   "source": [
    "### Task 5: POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d5393d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - POS Tagging:\n",
      "[('The', 'DT'), ('Fulton', 'NNP'), ('County', 'NNP'), ('Grand', 'NNP'), ('Jury', 'NNP'), ('said', 'VBD'), ('Friday', 'NNP'), ('an', 'DT'), ('investigation', 'NN'), ('of', 'IN'), ('Atlanta', 'NNP'), (\"'s\", 'POS'), ('recent', 'JJ'), ('primary', 'JJ'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'DT'), ('evidence', 'NN'), ('``', '``'), ('that', 'IN'), ('any', 'DT'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.'), ('The', 'DT'), ('jury', 'NN'), ('further', 'RB'), ('said', 'VBD')]\n",
      "\n",
      "Reuters - POS Tagging:\n",
      "[('ASIAN', 'NNP'), ('EXPORTERS', 'NNP'), ('FEAR', 'NNP'), ('DAMAGE', 'NNP'), ('FROM', 'NNP'), ('U', 'NNP'), ('.', '.'), ('S', 'NNP'), ('.-', 'JJ'), ('JAPAN', 'NNP'), ('RIFT', 'NNP'), ('Mounting', 'NNP'), ('trade', 'NN'), ('friction', 'NN'), ('between', 'IN'), ('the', 'DT'), ('U', 'NNP'), ('.', '.'), ('S', 'NNP'), ('.', '.'), ('And', 'CC'), ('Japan', 'NNP'), ('has', 'VBZ'), ('raised', 'VBN'), ('fears', 'NNS'), ('among', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('Asia', 'NNP'), (\"'\", 'POS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "brown_pos = pos_tag(brown_word_tokens[:30])\n",
    "reuters_pos = pos_tag(reuters_word_tokens[:30])\n",
    "\n",
    "print(\"Brown - POS Tagging:\")\n",
    "print(brown_pos)\n",
    "\n",
    "print(\"\\nReuters - POS Tagging:\")\n",
    "print(reuters_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac48dcc",
   "metadata": {},
   "source": [
    "### Task 6: Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e680e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Removing Punctuation:\n",
      "Original: 302 tokens\n",
      "After removal: 283 tokens\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', '``']\n",
      "\n",
      "Reuters - Removing Punctuation:\n",
      "Original: 305 tokens\n",
      "After removal: 263 tokens\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', 'S', 'And', 'Japan', 'has']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "brown_no_punct = [w for w in brown_word_tokens if w not in string.punctuation]\n",
    "reuters_no_punct = [w for w in reuters_word_tokens if w not in string.punctuation]\n",
    "\n",
    "print(\"Brown - Removing Punctuation:\")\n",
    "print(f\"Original: {len(brown_word_tokens)} tokens\")\n",
    "print(f\"After removal: {len(brown_no_punct)} tokens\")\n",
    "print(brown_no_punct[:20])\n",
    "\n",
    "print(\"\\nReuters - Removing Punctuation:\")\n",
    "print(f\"Original: {len(reuters_word_tokens)} tokens\")\n",
    "print(f\"After removal: {len(reuters_no_punct)} tokens\")\n",
    "print(reuters_no_punct[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5ea95",
   "metadata": {},
   "source": [
    "### Task 7: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8f5c779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Lowercasing:\n",
      "Original: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election']\n",
      "Lowercased: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlanta', \"'s\", 'recent', 'primary', 'election']\n",
      "\n",
      "Reuters - Lowercasing:\n",
      "Original: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between']\n",
      "Lowercased: ['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between']\n"
     ]
    }
   ],
   "source": [
    "brown_lower = [w.lower() for w in brown_word_tokens[:30]]\n",
    "reuters_lower = [w.lower() for w in reuters_word_tokens[:30]]\n",
    "\n",
    "print(\"Brown - Lowercasing:\")\n",
    "print(\"Original:\", brown_word_tokens[:15])\n",
    "print(\"Lowercased:\", brown_lower[:15])\n",
    "\n",
    "print(\"\\nReuters - Lowercasing:\")\n",
    "print(\"Original:\", reuters_word_tokens[:15])\n",
    "print(\"Lowercased:\", reuters_lower[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a108a1",
   "metadata": {},
   "source": [
    "### Task 8: Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e8615f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown - Split:\n",
      "Total tokens: 300\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "\n",
      "Reuters - Split:\n",
      "Total tokens: 300\n",
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.']\n"
     ]
    }
   ],
   "source": [
    "brown_split = brown_sample.split()\n",
    "reuters_split = reuters_sample.split()\n",
    "\n",
    "print(\"Brown - Split:\")\n",
    "print(f\"Total tokens: {len(brown_split)}\")\n",
    "print(brown_split[:20])\n",
    "\n",
    "print(\"\\nReuters - Split:\")\n",
    "print(f\"Total tokens: {len(reuters_split)}\")\n",
    "print(reuters_split[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8765bf0",
   "metadata": {},
   "source": [
    "### Task 9: Addiional function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa2373",
   "metadata": {},
   "source": [
    "### Task 10: Wordnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
